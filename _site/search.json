[
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "",
    "text": "In this study, we apply appropriate spatial point patterns analysis methods to discover the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore and describe the spatial patterns revealed by kernel density maps."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#aspatial-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#aspatial-data",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Aspatial data",
    "text": "Aspatial data\n\nGrab Posisi data\nFirst, the data needs to be imported into RStudio. In this exercise, we will need to import all of the data in order to observe the temporal distribution.\n\ndf <- read_parquet(file=\"data/aspatial/GrabPosisi/part-00000.parquet\")\ndf1 <- read_parquet(file=\"data/aspatial/GrabPosisi/part-00001.parquet\")\ndf2 <- read_parquet(file=\"data/aspatial/GrabPosisi/part-00002.parquet\")\ndf3 <- read_parquet(file=\"data/aspatial/GrabPosisi/part-00003.parquet\")\ndf4 <- read_parquet(file=\"data/aspatial/GrabPosisi/part-00004.parquet\")\ndf5 <- read_parquet(file=\"data/aspatial/GrabPosisi/part-00005.parquet\")\ndf6 <- read_parquet(file=\"data/aspatial/GrabPosisi/part-00006.parquet\")\ndf7 <- read_parquet(file=\"data/aspatial/GrabPosisi/part-00007.parquet\")\ndf8 <- read_parquet(file=\"data/aspatial/GrabPosisi/part-00008.parquet\")\ndf9 <- read_parquet(file=\"data/aspatial/GrabPosisi/part-00009.parquet\")\n\n\nCombining the data\nAll of the data covers 2 weeks of Grab rides, but they are jumbled up. In order to accurately extract the origin and destination points, we need to combine everything into a singular data frame.\n\nall_grab <- df %>%\n  full_join(df1) %>%\n  full_join(df2) %>%\n  full_join(df3) %>%\n  full_join(df4) %>%\n  full_join(df5) %>%\n  full_join(df6) %>%\n  full_join(df7) %>%\n  full_join(df8) %>%\n  full_join(df9)\n\n\n\nData handling: Converting the data type of pingtimestamp to date-time\nGrab marks the date-time of each data point as a pingtimestamp. As a result, we will need to transform the data type to date-time using lubridate.\n\nall_grab$pingtimestamp <- as_datetime(all_grab$pingtimestamp)\n\n\n\nData handling: Extracting trip origins\nNow, we extract the trip origin locations and derive 3 new columns for weekday, starting hour and day of month into a new data frame. The origin locations are derived by grouping trips according to their trj_id, arranging by the date-time and filtering for the first item in each group.\n\norigin_df <- all_grab %>%\n  group_by(trj_id) %>%\n  arrange(pingtimestamp) %>%\n  filter(row_number()==1) %>%\n  mutate(weekday = wday(pingtimestamp,\n                        label = TRUE,\n                        abbr = TRUE),\n         start_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\nData handling: Extracting destinations\nTo extract trip destinations, we use a similar code as above except we filter to take the nth item out of n items in a group.\n\ndest_df <- all_grab %>%\n  group_by(trj_id) %>%\n  arrange(pingtimestamp) %>%\n  filter(row_number()==n()) %>%\n  mutate(weekday = wday(pingtimestamp,\n                        label = TRUE,\n                        abbr = TRUE),\n         start_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\n\nWriting data as rds\nWith the data transformation done, we can now write it out as an rds for future use.\n\nwrite_rds(origin_df, \"data/rds/grab_origins.rds\")\nwrite_rds(dest_df, \"data/rds/grab_dest.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#geospatial-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#geospatial-data",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Geospatial data",
    "text": "Geospatial data\n\nSingapore Coastal outline (excluding islands)\nThis layer is derived from the Master Plan 2019 Subzone Boundary (No Sea) from data.gov. First, it needs to be imported into rstudio using st_read()\n\nmpsz_sf <- st_read(dsn = \"data/geospatial\", \n                layer = \"MPSZ-2019\")\n\nFrom the summary, it can be seen that the layer is projected to WGS84. To continue, there is the need to reproject the polygons to the correct CRS SVY21.\n\nmpsz3414_sf <- st_transform(mpsz_sf, 3414)\n\nNow, we can plot the map and view the data.\n\nplot(mpsz3414_sf[\"SUBZONE_N\"])\n\nThere are some islands in the map that lack bridges for Grab drivers to reach, such as Coney Island. However, there are other relevant islands such as Sentosa that need to be kept.\nFrom research, Grab drivers can be observed to be able to enter Sentosa and Jurong Island to pick up and drop off passengers at the current moment. However, the Jurong Island polygon in mpsz3414_sf is combined with another island Bukom. Furthermore, Grab drivers are only allowed onto the island if they have a security pass, resulting in few drivers entering and exiting the island. Therefore, I am choosing to remove Jurong Island and Bukom from the map.\nHence, the islands that need to be excluded from the map are:\n\nConey Island\nSouthern Group\nNorth-Eastern islands (including Pulau Ubin: Grab drivers can only pick up and drop off passengers at the ferry terminal)\nSudong\nSemakau\nJurong Island and Bukom\n\nTo remove these, I’ve chosen to filter them out by name. This is because Sentosa and Jurong Island are also labelled as “islands” and would otherwise be caught.\n\nmain_island <- mpsz3414_sf %>%\n              filter(SUBZONE_N !=\"CONEY ISLAND\") %>%\n              filter(SUBZONE_N != \"NORTH-EASTERN ISLANDS\") %>%\n              filter(SUBZONE_N != \"SOUTHERN GROUP\") %>%\n              filter(SUBZONE_N != \"SUDONG\") %>%\n              filter(SUBZONE_N != \"SEMAKAU\") %>%\n              filter(SUBZONE_N != \"SUDONG\") %>%\n              filter(SUBZONE_N != \"JURONG ISLAND AND BUKOM\")\n\nplot(main_island[\"SUBZONE_N\"])\n\nNow, we can use st_union to get the outline of Singapore.\n\nplot(st_union(main_island))\n\n\nWriting data as rds\nWith this, we can now save the data in the rds format.\n\nwrite_rds(main_island, \"data/rds/islandOutline.rds\")\n\n\n\n\nSingapore road network\nIn order to conduct Network Kernel Density Estimation (NKDE), we will need a road network to map points onto. For Singapore’s road network, we can get data from OpenStreetMap via Geofabrik. From the documentation that comes with the download, we know that the road network is found in the gis_osm_roads_free_1 shape file.\n\nroads_all <- st_read(dsn = \"data/geospatial/openstreetmap\", \n                layer = \"gis_osm_roads_free_1\")\n\nThis data is also not projected to SVY21, which is something we will have to fix.\n\nroads_all <- st_transform(roads_all, 3414)\n\nOne issue currently is that the data includes all roads from Singapore, Malaysia and Brunei. We only want the road network for Singapore. In order to extract this, we can use st_intersection to filter for roads within Singapore based on the CoastalOutline layer (as imported from rds)\n\nsg_roads <- st_intersection(roads_all, CoastalOutline)\n\nDue to the size of the data, we will later be narrowing the scope of our investigation for NKDE to certain subzones. For now, I will save the current road network into an rds.\n\nwrite_rds(sg_roads, \"data/rds/sgroads.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#analysis-of-origins",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#analysis-of-origins",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Analysis of origins",
    "text": "Analysis of origins\nFirst, lets look at the distribution of origins over time using ggplot.\n\nggplot(data = origin_all_df,\n       aes(x=weekday)) +\ngeom_bar()\n\n\n\n\nAs seen above, the data is evenly distributed across all days of the week.\n\nppp object transformation\nIn order to conduct Kernel Density Estimation (KDE), we need to transform our layers into a ppp object. Before we begin that process, we will need to transform origin_all_df into an sf object using st_as_sf().\n\n\n\n\n\n\nImportant\n\n\n\nFor the “crs” argument of st_as_sf(), we need to provide it with a geodetic CRS and not a projected CRS. To transform the CRS, we can pipe the output into an st_transform() argument.\n\n\n\norigins_sf <- st_as_sf(origin_all_df, \n                       coords = c(\"rawlng\", \"rawlat\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\nNow, we can transform all relevant data frames into a ppp object (via transforming to Spatial/*, SpatialPoints and then ppp objects), which we can inspect using the summary function.\n\norigin_spatial <- as_Spatial(origins_sf)\norigin_sp <- as(origin_spatial, \"SpatialPoints\")\norigin_ppp <- as(origin_sp, \"ppp\")\n\nsummary(origin_ppp)\n\nPlanar point pattern:  28000 points\nAverage intensity 2.473666e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3628.24, 49845.23] x [25198.14, 49689.64] units\n                    (46220 x 24490 units)\nWindow area = 1131920000 square units\n\n\n\n\nDuplicate data handling\nNow, we need to check for duplicated data.\n\nany(duplicated(origin_ppp))\n\n[1] FALSE\n\n\nIf we want to know how many locations have more than one point event, we can use the following:\n\nsum(multiplicity(origin_ppp) > 1)\n\n[1] 0\n\n\nAs we can see, there is no duplicate data in the ppp object.\n\n\nCreating owin object\nNext, we need to confine the analysis with a geographical area like Singapore boundary. If a study area is not defined and confined, the data points will assume it can occur within blank spaces (because technically it will spread out at random).\n\nCoastalOutline <- st_union(CoastalOutline)\nsg_owin <- as.owin(CoastalOutline)\nplot(sg_owin)\n\n\n\n\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (35 holes)\n                  vertices         area relative.area\npolygon 1            13910  6.66691e+08      9.98e-01\npolygon 2              285  1.61128e+06      2.41e-03\npolygon 3               27  1.50315e+04      2.25e-05\npolygon 4 (hole)        41 -4.01660e+04     -6.01e-05\npolygon 5 (hole)       317 -5.11280e+04     -7.65e-05\npolygon 6 (hole)         3 -4.14099e-04     -6.20e-13\npolygon 7               30  2.80002e+04      4.19e-05\npolygon 8 (hole)         4 -2.86396e-01     -4.29e-10\npolygon 9 (hole)         3 -1.81439e-04     -2.71e-13\npolygon 10 (hole)        3 -8.68789e-04     -1.30e-12\npolygon 11 (hole)        3 -5.99535e-04     -8.97e-13\npolygon 12 (hole)        3 -3.04561e-04     -4.56e-13\npolygon 13 (hole)        3 -4.46076e-04     -6.67e-13\npolygon 14 (hole)        3 -3.39794e-04     -5.08e-13\npolygon 15 (hole)        3 -4.52043e-05     -6.76e-14\npolygon 16 (hole)        3 -3.90173e-05     -5.84e-14\npolygon 17 (hole)        3 -9.59850e-05     -1.44e-13\npolygon 18 (hole)        4 -2.54488e-04     -3.81e-13\npolygon 19 (hole)        4 -4.28453e-01     -6.41e-10\npolygon 20 (hole)        4 -2.18616e-04     -3.27e-13\npolygon 21 (hole)        5 -2.44411e-04     -3.66e-13\npolygon 22 (hole)        5 -3.64686e-02     -5.46e-11\npolygon 23              71  8.18750e+03      1.23e-05\npolygon 24 (hole)        6 -8.37554e-01     -1.25e-09\npolygon 25 (hole)       38 -7.79904e+03     -1.17e-05\npolygon 26 (hole)        3 -3.41897e-05     -5.12e-14\npolygon 27 (hole)        3 -3.65499e-03     -5.47e-12\npolygon 28 (hole)        3 -4.95057e-02     -7.41e-11\npolygon 29              91  1.49663e+04      2.24e-05\npolygon 30 (hole)        5 -2.92235e-04     -4.37e-13\npolygon 31 (hole)        3 -7.43616e-06     -1.11e-14\npolygon 32 (hole)      270 -1.21455e+03     -1.82e-06\npolygon 33 (hole)       19 -4.39650e+00     -6.58e-09\npolygon 34 (hole)       35 -1.38385e+02     -2.07e-07\npolygon 35 (hole)       23 -1.99656e+01     -2.99e-08\npolygon 36              40  1.38607e+04      2.07e-05\npolygon 37 (hole)       41 -6.00381e+03     -8.98e-06\npolygon 38 (hole)        7 -1.40546e-01     -2.10e-10\npolygon 39 (hole)       11 -8.36705e+01     -1.25e-07\npolygon 40 (hole)        3 -2.33435e-03     -3.49e-12\npolygon 41              45  2.51218e+03      3.76e-06\npolygon 42             139  3.22293e+03      4.82e-06\npolygon 43             148  3.10395e+03      4.64e-06\npolygon 44 (hole)        4 -1.72650e-04     -2.58e-13\npolygon 45              75  1.73526e+04      2.60e-05\npolygon 46              83  5.28920e+03      7.91e-06\npolygon 47             106  3.04104e+03      4.55e-06\npolygon 48              71  5.63061e+03      8.43e-06\npolygon 49              10  1.99717e+02      2.99e-07\npolygon 50 (hole)        3 -1.37223e-02     -2.05e-11\nenclosing rectangle: [2667.54, 55941.94] x [21448.47, 50256.33] units\n                     (53270 x 28810 units)\nWindow area = 668316000 square units\nFraction of frame area: 0.435\n\n\nWith that, we can finally plot the point data onto the map.\n\noriginSG = origin_ppp[sg_owin]\n\nplot(originSG)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#rescaling-values",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#rescaling-values",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Rescaling values",
    "text": "Rescaling values\nBefore we can conduct KDE analysis, we first need to rescale the unit of measurement in originSG from metres (the unit of measurement in SVY21) to kilometres (to avoid small numbers).\n\noriginSG_km <- rescale(originSG, 1000, \"km\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#working-with-different-automatic-bandwidth-selection-methods",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#working-with-different-automatic-bandwidth-selection-methods",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Working with different automatic bandwidth selection methods",
    "text": "Working with different automatic bandwidth selection methods\nThere are 4 different automatic bandwidth selection method that we can choose: bw.diggle, bw.CvL, bw.scott and bw.ppl. For this exercise, we are using bw.diggle, or the radius selection method. We can plot all four to see which may give us better results.\n\nkde_origins_dig <- density(originSG_km,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\nkde_origins_ppl <- density(originSG_km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\n\nkde_origins_cvl <- density(originSG_km,\n                              sigma=bw.CvL,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\nkde_origins_scott <- density(originSG_km,\n                              sigma=bw.scott,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\npar(mfrow=c(2,2))\nplot(kde_origins_dig, main = \"bw.diggle\")\nplot(kde_origins_ppl, main = \"bw.ppl\")\nplot(kde_origins_cvl, main = \"bw.cvl\")\nplot(kde_origins_scott, main = \"bw.scott\")\n\n\n\n\nWe can retrieve the bandwidth used to compute the KDE layer as well.\n\nbw.diggle(originSG)\nbw.ppl(originSG)\nbw.CvL(originSG)\nbw.scott(originSG)\n\nAs seen, bw.ppl and bw.diggle appears to give relatively similar map results which highlight a single tight cluster in the east and include up to a thousand origin points. Meanwhile, bw.cvl and bw.scott appear to use tighter radii, with bw.scott being able to better show hotspots compared to how spread out bw.cvl looks. Hence, for the rest of this exercise, we will be using bw.scott."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#working-with-different-kernel-methods",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#working-with-different-kernel-methods",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Working with different kernel methods",
    "text": "Working with different kernel methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics. We can test each of these to see which kernel method provides a tighter grouping.\n\npar(mfrow=c(2,2))\nplot(density(originSG_km, \n             sigma=bw.scott, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\n\nplot(density(originSG_km, \n             sigma=bw.scott, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\n\nplot(density(originSG_km, \n             sigma=bw.scott, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\n\nplot(density(originSG_km, \n             sigma=bw.scott, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")\n\n\n\n\nFor this exercise, disc and quartic appear to provide us with tighter values. Hence, we will be using Disc."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#conversion-to-raster-via-grid-object",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#conversion-to-raster-via-grid-object",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Conversion to raster via grid object",
    "text": "Conversion to raster via grid object\nIn order to fully visualise KDE in tmap, we would need to transform it into raster data. Before that, we need to convert it into a grid object\n\ngridded_kde_origins_bw <- as.SpatialGridDataFrame.im(kde_originSG_600)\nspplot(gridded_kde_origins_bw)\n\n\n\n\nNow, we can convert it to raster data.\n\nkde_originsSG_bw_raster <- raster(gridded_kde_origins_bw)\n\nNote that there is no CRS to this data, so we will need to assign it on our own\n\nprojection(kde_originsSG_bw_raster) <- CRS(\"+init=EPSG:3414\")\nkde_originsSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4162063, 0.2250614  (x, y)\nextent     : 2.667538, 55.94194, 21.44847, 50.25633  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : v \nvalues     : -1.173372e-13, 353.656  (min, max)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#visualising-kde-tmap",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#visualising-kde-tmap",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Visualising KDE (tmap)",
    "text": "Visualising KDE (tmap)\n\ntm_shape(kde_originsSG_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#ppp-object-transformation",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#ppp-object-transformation",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "ppp object transformation",
    "text": "ppp object transformation\nIn order to conduct Kernel Density Estimation (KDE), we need to transform our layers into a ppp object. Before we begin that process, we will need to transform origin_all_df into an sf object using st_as_sf().\n\n\n\n\n\n\nImportant\n\n\n\nFor the “crs” argument of st_as_sf(), we need to provide it with a geodetic CRS and not a projected CRS. To transform the CRS, we can pipe the output into an st_transform() argument.\n\n\n\norigins_sf <- st_as_sf(origin_all_df, \n                       coords = c(\"rawlng\", \"rawlat\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\nNow, we can transform all relevant data frames into a ppp object (via transforming to Spatial/*, SpatialPoints and then ppp objects), which we can inspect using the summary function.\n\norigin_spatial <- as_Spatial(origins_sf)\norigin_sp <- as(origin_spatial, \"SpatialPoints\")\norigin_ppp <- as(origin_sp, \"ppp\")\n\nsummary(origin_ppp)\n\nPlanar point pattern:  28000 points\nAverage intensity 2.473666e-05 points per square unit\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [3628.24, 49845.23] x [25198.14, 49689.64] units\n                    (46220 x 24490 units)\nWindow area = 1131920000 square units"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#duplicate-data-handling",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#duplicate-data-handling",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Duplicate data handling",
    "text": "Duplicate data handling\nNow, we need to check for duplicated data.\n\nany(duplicated(origin_ppp))\n\n[1] FALSE\n\n\nIf we want to know how many locations have more than one point event, we can use the following:\n\nsum(multiplicity(origin_ppp) > 1)\n\n[1] 0\n\n\nAs we can see, there is no duplicate data in the ppp object."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#creating-owin-object",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#creating-owin-object",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Creating owin object",
    "text": "Creating owin object\nNext, we need to confine the analysis with a geographical area like Singapore boundary. If a study area is not defined and confined, the data points will assume it can occur within blank spaces (because technically it will spread out at random).\n\nCoastalOutlineSG <- st_union(CoastalOutline)\nsg_owin <- as.owin(CoastalOutlineSG)\nplot(sg_owin)\n\n\n\n\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n50 separate polygons (35 holes)\n                  vertices         area relative.area\npolygon 1            13910  6.66691e+08      9.98e-01\npolygon 2              285  1.61128e+06      2.41e-03\npolygon 3               27  1.50315e+04      2.25e-05\npolygon 4 (hole)        41 -4.01660e+04     -6.01e-05\npolygon 5 (hole)       317 -5.11280e+04     -7.65e-05\npolygon 6 (hole)         3 -4.14099e-04     -6.20e-13\npolygon 7               30  2.80002e+04      4.19e-05\npolygon 8 (hole)         4 -2.86396e-01     -4.29e-10\npolygon 9 (hole)         3 -1.81439e-04     -2.71e-13\npolygon 10 (hole)        3 -8.68789e-04     -1.30e-12\npolygon 11 (hole)        3 -5.99535e-04     -8.97e-13\npolygon 12 (hole)        3 -3.04561e-04     -4.56e-13\npolygon 13 (hole)        3 -4.46076e-04     -6.67e-13\npolygon 14 (hole)        3 -3.39794e-04     -5.08e-13\npolygon 15 (hole)        3 -4.52043e-05     -6.76e-14\npolygon 16 (hole)        3 -3.90173e-05     -5.84e-14\npolygon 17 (hole)        3 -9.59850e-05     -1.44e-13\npolygon 18 (hole)        4 -2.54488e-04     -3.81e-13\npolygon 19 (hole)        4 -4.28453e-01     -6.41e-10\npolygon 20 (hole)        4 -2.18616e-04     -3.27e-13\npolygon 21 (hole)        5 -2.44411e-04     -3.66e-13\npolygon 22 (hole)        5 -3.64686e-02     -5.46e-11\npolygon 23              71  8.18750e+03      1.23e-05\npolygon 24 (hole)        6 -8.37554e-01     -1.25e-09\npolygon 25 (hole)       38 -7.79904e+03     -1.17e-05\npolygon 26 (hole)        3 -3.41897e-05     -5.12e-14\npolygon 27 (hole)        3 -3.65499e-03     -5.47e-12\npolygon 28 (hole)        3 -4.95057e-02     -7.41e-11\npolygon 29              91  1.49663e+04      2.24e-05\npolygon 30 (hole)        5 -2.92235e-04     -4.37e-13\npolygon 31 (hole)        3 -7.43616e-06     -1.11e-14\npolygon 32 (hole)      270 -1.21455e+03     -1.82e-06\npolygon 33 (hole)       19 -4.39650e+00     -6.58e-09\npolygon 34 (hole)       35 -1.38385e+02     -2.07e-07\npolygon 35 (hole)       23 -1.99656e+01     -2.99e-08\npolygon 36              40  1.38607e+04      2.07e-05\npolygon 37 (hole)       41 -6.00381e+03     -8.98e-06\npolygon 38 (hole)        7 -1.40546e-01     -2.10e-10\npolygon 39 (hole)       11 -8.36705e+01     -1.25e-07\npolygon 40 (hole)        3 -2.33435e-03     -3.49e-12\npolygon 41              45  2.51218e+03      3.76e-06\npolygon 42             139  3.22293e+03      4.82e-06\npolygon 43             148  3.10395e+03      4.64e-06\npolygon 44 (hole)        4 -1.72650e-04     -2.58e-13\npolygon 45              75  1.73526e+04      2.60e-05\npolygon 46              83  5.28920e+03      7.91e-06\npolygon 47             106  3.04104e+03      4.55e-06\npolygon 48              71  5.63061e+03      8.43e-06\npolygon 49              10  1.99717e+02      2.99e-07\npolygon 50 (hole)        3 -1.37223e-02     -2.05e-11\nenclosing rectangle: [2667.54, 55941.94] x [21448.47, 50256.33] units\n                     (53270 x 28810 units)\nWindow area = 668316000 square units\nFraction of frame area: 0.435\n\n\nWith that, we can finally plot the point data onto the map.\n\noriginSG = origin_ppp[sg_owin]\n\nplot(originSG)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#fixed-and-adaptive-kde-bandwidths",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#fixed-and-adaptive-kde-bandwidths",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Fixed and Adaptive KDE bandwidths",
    "text": "Fixed and Adaptive KDE bandwidths\nFor a fixed bandwidth, we will be fixing the radius at 600m.\n\nkde_originSG_600 <- density(originSG_km, sigma=0.6, edge=TRUE, kernel=\"disc\")\nplot(kde_originSG_600)\n\n\n\n\nTypically, a fixed bandwidth method is very sensitive to highly skewed distribution of spatial point patterns over geographical units. To get around this, we can use an adaptive bandwidth instead.\n\nkde_originSG_adaptive <- adaptive.density(originSG_km, method=\"kernel\")\nplot(kde_originSG_adaptive)\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(kde_originSG_600, main = \"Fixed bandwidth\")\nplot(kde_originSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\nConversion to raster via grid object\nIn order to fully visualise KDE in tmap, we would need to transform it into raster data. Before that, we need to convert it into a grid object\n\ngridded_kde_origins_bw <- as.SpatialGridDataFrame.im(kde_originSG_600)\nspplot(gridded_kde_origins_bw)\n\n\n\n\nNow, we can convert it to raster data.\n\nkde_originsSG_bw_raster <- raster(gridded_kde_origins_bw)\n\nNote that there is no CRS to this data, so we will need to assign it on our own\n\nprojection(kde_originsSG_bw_raster) <- CRS(\"+init=EPSG:3414\")\nkde_originsSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.4162063, 0.2250614  (x, y)\nextent     : 2.667538, 55.94194, 21.44847, 50.25633  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : v \nvalues     : -1.173372e-13, 353.656  (min, max)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#extracting-study-areas",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#extracting-study-areas",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Extracting study areas",
    "text": "Extracting study areas\nHaving seen the traditional KDE of Grab origin points across, Singapore, we can identify the subzones where hotspots are located at and extract them for closer analysis.\n\npar(mfrow=c(1,2))\nplot(gridded_kde_origins_bw)\n\n\n\nplot(CoastalOutline[\"SUBZONE_N\"])"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#spatial-pattern-observations",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#spatial-pattern-observations",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Spatial pattern observations",
    "text": "Spatial pattern observations\nFrom the raster map of the island, we can make the following observations.\n\nThere are 5 major clusters where Grab drivers begin their trips from\nThe cluster with the most observations is located in the east, spread across Tampines and Changi planning areas\nThe cluster with the second most observations is located in the central area, largely covering the Downtown Core\nThe other three clusters cover Jurong East and West, Choa Chu Kang, and Woodlands planning areas"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#comparing-spatial-point-patterns",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#comparing-spatial-point-patterns",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Comparing Spatial Point patterns",
    "text": "Comparing Spatial Point patterns\n\nExtracting study areas\nHaving seen the traditional KDE of Grab origin points across, Singapore, we can identify the planning areas where hotspots are located and extract them for closer analysis.\nBased on the raster data, I will be extracting the following planning areas:\n\nCHANGI\nTAMPINES\nDOWNTOWN CORE\nWOODLANDS\n\nNote that to do this, we do need CoastalOutline layer to be a SpatialObject first.\n\ncc = CoastalOutline[CoastalOutline$PLN_AREA_N == \"CHOA CHU KANG\",]\ndt = CoastalOutline[CoastalOutline$PLN_AREA_N == \"DOWNTOWN CORE\",]\nwl = CoastalOutline[CoastalOutline$PLN_AREA_N == \"WOODLANDS\",]\ntp = CoastalOutline[CoastalOutline$PLN_AREA_N == \"TAMPINES\",]\n\nNext, we can convert these objects into owin objects via the same process as before\n\ncc_spa <- as_Spatial(cc)\ntp_spa <- as_Spatial(tp)\nwl_spa <- as_Spatial(wl)\ndt_spa <- as_Spatial(dt)\n\ncc_sp = as(cc_spa, \"SpatialPolygons\")\ntp_sp = as(tp_spa, \"SpatialPolygons\")\nwl_sp = as(wl_spa, \"SpatialPolygons\")\ndt_sp = as(dt_spa, \"SpatialPolygons\")\n\ncc_owin = as(cc_sp, \"owin\")\ntp_owin = as(tp_sp, \"owin\")\nwl_owin = as(wl_sp, \"owin\")\ndt_owin = as(dt_sp, \"owin\")\n\nFollowing this, we combine it to the origin_ppp layer to extract relevant Grab data.\n\norigins_cc_owin = origin_ppp[cc_owin]\norigins_tp_owin = origin_ppp[tp_owin]\norigins_wl_owin = origin_ppp[wl_owin]\norigins_dt_owin = origin_ppp[dt_owin]\n\nNext, rescale() function is used to trasnform the unit of measurement from metre to kilometre\n\norigins_cc_owin_km = rescale(origins_cc_owin, 1000, \"km\")\norigins_tp_owin_km = rescale(origins_tp_owin, 1000, \"km\")\norigins_wl_owin_km = rescale(origins_wl_owin, 1000, \"km\")\norigins_dt_owin_km = rescale(origins_dt_owin, 1000, \"km\")\n\nWe can now plot the four study areas again to see the spatial distribution of Grab origin points.\n\npar(mfrow=c(2,2))\nplot(origins_cc_owin_km, main=\"Choa Chu Kang\")\nplot(origins_tp_owin_km, main=\"Tampines\")\nplot(origins_wl_owin_km, main=\"Woodlands\")\nplot(origins_dt_owin_km, main=\"Downtown Core\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#analysing-study-areas",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#analysing-study-areas",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Analysing study areas",
    "text": "Analysing study areas\nNow, we can compute the KDE of each planning areas using the methods we used for the entirety of Singapore.\nNote that we are using bw.scott for this instead of bw.diggle.\n\n\nCode\npar(mfrow=c(2,2))\nplot(density(origins_cc_owin_km, \n             sigma=bw.scott, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\n\nplot(density(origins_tp_owin_km, \n             sigma=bw.scott, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\nplot(density(origins_wl_owin_km, \n             sigma=bw.scott, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Woodlands\")\n\nplot(density(origins_dt_owin_km, \n             sigma=bw.scott, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Downtown Core\")\n\n\n\n\n\nWe can also compute the fixed bandwidth for these planning areas. For this we will be using a tighter bandwidth of 300m, or half of the previously used bandwidth.\n\npar(mfrow=c(2,2))\nplot(density(origins_cc_owin_km, \n             sigma=0.3, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\n\nplot(density(origins_tp_owin_km, \n             sigma=0.3, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")\n\nplot(density(origins_wl_owin_km, \n             sigma=0.3, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Woodlands\")\n\nplot(density(origins_dt_owin_km, \n             sigma=0.3, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Downtown Core\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#confirmatory-analysis-nearest-neighbour",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#confirmatory-analysis-nearest-neighbour",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Confirmatory analysis: Nearest Neighbour",
    "text": "Confirmatory analysis: Nearest Neighbour\nOne way that we can analyse and describe the distribution of spatial point patterns is via the Nearest Neighbour analysis using clarkevans.test() of statspat.\nFor this to work, we will be using the following hypothesis:\nHo = The distribution of Grab ride origins are randomly distributed.\nH1= The distribution of Grab ride origins are not randomly distributed.\nThe 95% confident interval will be used.\n\nClark and Evans Test: whole island\n\nclarkevans.test(originSG,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  originSG\nR = 0.28003, p-value < 2.2e-16\nalternative hypothesis: clustered (R < 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause p-value < 0.05, we reject H0. This means that the data is not randomly distributed spatially.\n\n\n\n\nClark and Evans Test: Choa Chu Kang\n\nclarkevans.test(origins_cc_owin,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  origins_cc_owin\nR = 0.34217, p-value < 2.2e-16\nalternative hypothesis: clustered (R < 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause p-value < 0.05, we reject H0. This means that the data is not randomly distributed spatially.\n\n\n\n\nClark and Evans Test: Tampines\n\nclarkevans.test(origins_tp_owin,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  origins_tp_owin\nR = 0.32408, p-value < 2.2e-16\nalternative hypothesis: clustered (R < 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause p-value < 0.05, we reject H0. This means that the data is not randomly distributed spatially.\n\n\n\n\nClark and Evans Test: Woodlands\n\nclarkevans.test(origins_wl_owin,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  origins_wl_owin\nR = 0.31779, p-value < 2.2e-16\nalternative hypothesis: clustered (R < 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause p-value < 0.05, we reject H0. This means that the data is not randomly distributed spatially.\n\n\n\n\nClark and Evans Test: Downtown Core\n\nclarkevans.test(origins_dt_owin,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  origins_dt_owin\nR = 0.47245, p-value < 2.2e-16\nalternative hypothesis: clustered (R < 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause p-value < 0.05, we reject H0. This means that the data is not randomly distributed spatially."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-wrangling",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Data wrangling",
    "text": "Data wrangling\nIn order to conduct Network Kernel Density Estimation (NKDE), we will need a road network. For this section, we will be doing this analysis by planning area.\nFirstly, we should clean up the data before extracting according to planning area. According to the document that is attached to the data from OpenStreetMap, some of the roads labelled are not roads that cars can travel on, such as pedestrian walkways. We will need to filter out the following classes from the fclass column:\n\npedestrian\nbusway\nbridleway\ncycleway\nfootway\npath\nsteps\n\n\nsg_roads_clean <- sg_roads %>%\n              filter(fclass !=\"pedestrian\") %>%\n              filter(fclass !=\"busway\") %>%\n              filter(fclass !=\"bridleway\") %>%\n              filter(fclass !=\"cycleway\") %>%\n              filter(fclass !=\"footway\") %>%\n              filter(fclass !=\"path\") %>%\n              filter(fclass !=\"steps\")\n\nNext, we can extract the roads for each planning area.\n\nroad_cc <- st_intersection(sg_roads_clean, cc)\nroad_tp <- st_intersection(sg_roads_clean, tp)\nroad_wl <- st_intersection(sg_roads_clean, wl)\nroad_dt <- st_intersection(sg_roads_clean, dt)\n\n\nroad_cc\n\nSimple feature collection with 3163 features and 16 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 16806.03 ymin: 39012.19 xmax: 19973.64 ymax: 43036.12\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n      osm_id code      fclass                   name  ref oneway maxspeed layer\n608 22721844 5113     primary      Choa Chu Kang Way <NA>      F       60     0\n611 22721847 5115    tertiary Choa Chu Kang Avenue 1 <NA>      F       50     0\n617 22732932 5122 residential Choa Chu Kang Avenue 2 <NA>      B       50     0\n619 22732934 5122 residential       Hong San Terrace <NA>      B       50     0\n620 22732935 5122 residential          Hong San Walk <NA>      F       50     0\n621 22732936 5122 residential          Hong San Walk <NA>      B       50     0\n686 22752008 5113     primary     Choa Chu Kang Road <NA>      F       60     0\n688 22752010 5113     primary      Choa Chu Kang Way <NA>      F       60     0\n689 22752011 5113     primary      Choa Chu Kang Way <NA>      F       60     0\n692 22752085 5113     primary         Brickland Road <NA>      F       60     0\n    bridge tunnel SUBZONE_N SUBZONE_C    PLN_AREA_N PLN_AREA_C    REGION_N\n608      F      F KEAT HONG    CKSZ02 CHOA CHU KANG         CK WEST REGION\n611      F      F KEAT HONG    CKSZ02 CHOA CHU KANG         CK WEST REGION\n617      F      F KEAT HONG    CKSZ02 CHOA CHU KANG         CK WEST REGION\n619      F      F KEAT HONG    CKSZ02 CHOA CHU KANG         CK WEST REGION\n620      F      F KEAT HONG    CKSZ02 CHOA CHU KANG         CK WEST REGION\n621      F      F KEAT HONG    CKSZ02 CHOA CHU KANG         CK WEST REGION\n686      F      F KEAT HONG    CKSZ02 CHOA CHU KANG         CK WEST REGION\n688      F      F KEAT HONG    CKSZ02 CHOA CHU KANG         CK WEST REGION\n689      F      F KEAT HONG    CKSZ02 CHOA CHU KANG         CK WEST REGION\n692      F      F KEAT HONG    CKSZ02 CHOA CHU KANG         CK WEST REGION\n    REGION_C                       geometry\n608       WR LINESTRING (19142.48 39802....\n611       WR LINESTRING (18158.17 39825....\n617       WR LINESTRING (18187.23 40237....\n619       WR LINESTRING (18287.68 39877....\n620       WR LINESTRING (18485.43 39930....\n621       WR LINESTRING (18457.67 39987....\n686       WR LINESTRING (19142.48 39802....\n688       WR LINESTRING (19151.49 39807....\n689       WR LINESTRING (19145.42 39799....\n692       WR LINESTRING (18564.37 39021....\n\n\nNote how it is geometry type geometry and not a linestring. We can convert them with the following code chunk:\n\nroad_cc <- road_cc %>%\n  st_cast(\"LINESTRING\")\n\nroad_tp <- road_tp %>%\n  st_cast(\"LINESTRING\")\n\nroad_wl <- road_wl %>%\n  st_cast(\"LINESTRING\")\n\nroad_dt <- road_dt %>%\n  st_cast(\"LINESTRING\")\n\nFinally, in order to plot the points onto a map, we will need to extract the Grab origin points separately.\n\norigin_cc <- st_intersection(origins_sf, cc)\norigin_tp <- st_intersection(origins_sf, tp)\norigin_wl <- st_intersection(origins_sf, wl)\norigin_dt <- st_intersection(origins_sf, dt)\n\nWe can now plot this on a map together with the owin containing all events within the subzone. Below is an example using Tampines planning area.\n\ntmap_mode('view')\ntm_shape(origin_tp)+\n  tm_dots() +\n  tm_shape(road_tp) +\n  tm_lines()\n\n\n\n\n\ntmap_mode('plot')\n\nWe can also plot the individual network.\n\nplot(road_tp[\"fclass\"])"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#preparing-lixels-object",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#preparing-lixels-object",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Preparing lixels object",
    "text": "Preparing lixels object\nTo conduct NKDE, we will need to convert the road network into a lixels object. To do this, we first need to segment the lines in a network.\n\n\n\n\n\n\nNote\n\n\n\nWhy use 750m? Based off a study by NUS that the maximum walking distance people are willing to walk is 750m. Since we use 750m, the middle distance is 375m (half of the bandwidth).\n\n\n\n#|code-fold: True\n\nlixel_cc <- lixelize_lines(road_cc,\n                         750,\n                         mindist = 375)\n\nlixel_tp <- lixelize_lines(road_tp,\n                         750,\n                         mindist = 375)\n\nlixel_wl <- lixelize_lines(road_wl,\n                         750,\n                         mindist = 375)\n\nlixel_dt <- lixelize_lines(road_dt,\n                         750,\n                         mindist = 375)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#generating-line-centre-points",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#generating-line-centre-points",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Generating line centre points",
    "text": "Generating line centre points\nNext, we generate the centroids of each line to ensure that they are consistent.\n\ncentroid_cc <- lines_center(lixel_cc)\ncentroid_tp <- lines_center(lixel_tp)\ncentroid_wl <- lines_center(lixel_wl)\ncentroid_dt <- lines_center(lixel_dt)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#performing-nkde",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#performing-nkde",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Performing NKDE",
    "text": "Performing NKDE\nWith both centroids and lixels, we can finally perform NKDE.\n\n\nCode\ndensity_cc <- nkde(road_cc, \n                  events = origin_cc,\n                  w = rep(1,nrow(origin_cc)),\n                  samples = centroid_cc,\n                  kernel_name = \"quartic\", \n                  bw = 300,\n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, #we aggregate events within a 5m radius (faster calculation)\n                  sparse = TRUE,\n                  verbose = FALSE)\n\ndensity_tp <- nkde(road_tp, \n                  events = origin_tp,\n                  w = rep(1,nrow(origin_tp)),\n                  samples = centroid_tp,\n                  kernel_name = \"quartic\", \n                  bw = 300,\n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, #we aggregate events within a 5m radius (faster calculation)\n                  sparse = TRUE,\n                  verbose = FALSE)\n\ndensity_wl <- nkde(road_wl, \n                  events = origin_wl,\n                  w = rep(1,nrow(origin_wl)),\n                  samples = centroid_wl,\n                  kernel_name = \"quartic\", \n                  bw = 300,\n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, #we aggregate events within a 5m radius (faster calculation)\n                  sparse = TRUE,\n                  verbose = FALSE)\n\ndensity_dt <- nkde(road_dt, \n                  events = origin_dt,\n                  w = rep(1,nrow(origin_dt)),\n                  samples = centroid_dt,\n                  kernel_name = \"quartic\", \n                  bw = 300,\n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, #we aggregate events within a 5m radius (faster calculation)\n                  sparse = TRUE,\n                  verbose = FALSE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#visualising-nkde",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#visualising-nkde",
    "title": "Take Home Exercise 1: Application of Spatial Point Patterns Analysis",
    "section": "Visualising NKDE",
    "text": "Visualising NKDE\nBefore we can visualise the NetKDE values, code chunk below will be used to insert the computed density values (i.e. densities) into samples and lixels objects as density field.\n\n\nCode\ncentroid_cc$density <- density_cc\nlixel_cc$density <- density_cc\n\ncentroid_tp$density <- density_tp\nlixel_tp$density <- density_tp\n\ncentroid_wl$density <- density_wl\nlixel_wl$density <- density_wl\n\ncentroid_dt$density <- density_dt\nlixel_dt$density <- density_dt\n\n\nNext, because svy21 is in meters, we will need to convert the numbers to kilometers to avoid small numbers.\n\n# rescaling to help the mapping\ncentroid_cc$density <- centroid_cc$density*1000\nlixel_cc$density <- lixel_cc$density*1000\n\ncentroid_tp$density <- centroid_tp$density*1000\nlixel_tp$density <- lixel_tp$density*1000\n\ncentroid_wl$density <- centroid_wl$density*1000\nlixel_wl$density <- lixel_wl$density*1000\n\ncentroid_dt$density <- centroid_dt$density*1000\nlixel_dt$density <- lixel_dt$density*1000\n\nNow, we can visualise the network. For the sake of being able to see the road network, I will not be plotting the point data.\n\ntmap_mode('view')\ntm_shape(lixel_cc)+\n  tm_lines(col=\"density\")\n\n\n\n\n\ntmap_mode('plot')\n\n\ntmap_mode('view')\ntm_shape(lixel_tp)+\n  tm_lines(col=\"density\")\n\n\n\n\n\ntmap_mode('plot')\n\n\ntmap_mode('view')\ntm_shape(lixel_wl)+\n  tm_lines(col=\"density\")\n\n\n\n\n\ntmap_mode('plot')\n\n\ntmap_mode('view')\ntm_shape(lixel_dt)+\n  tm_lines(col=\"density\")\n\n\n\n\n\ntmap_mode('plot')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IS415-GAA-2024",
    "section": "",
    "text": "This is the course website of IS415 I study this term. You will find my course work on this website."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "According to the exercise, we will learn how to compute Global Measures of Spatial Autocorrelation (GMSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\nprovide statistically correct interpretation of GSA statistics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#global-autocorrelation-as-taken-from-the-website",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#global-autocorrelation-as-taken-from-the-website",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Global autocorrelation (as taken from the website)",
    "text": "Global autocorrelation (as taken from the website)\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#local-autocorrelation-as-taken-from-the-website",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#local-autocorrelation-as-taken-from-the-website",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Local autocorrelation (as taken from the website)",
    "text": "Local autocorrelation (as taken from the website)\nIn spatial policy, one of the main development objective of the local govenment and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.(https://en.wikipedia.org/wiki/Hunan)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#importing-data",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Importing data",
    "text": "Importing data\nFirst, we read ithe Hunan shape file using st_read()\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\OdeliaPeh\\IS415-GAA-2024\\Hands-on_Ex\\Hands-on_Ex05\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#data-wrangling-relational-join",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#data-wrangling-relational-join",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Data wrangling: relational join",
    "text": "Data wrangling: relational join\nWe use left_join() to update the attribute table of the Hunan layer with some of the attributes from hunan2012 dataframe.\n\nhunan <- left_join(hunan,hunan2012) %>%\n  select(1:4, 7, 15)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-regional-development-indicator",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#visualising-regional-development-indicator",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Visualising Regional Development Indicator",
    "text": "Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#morans-i-test",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#morans-i-test",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Moran’s I Test",
    "text": "Moran’s I Test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\nComputing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\nVisualising Monte Carlo Moran’s I\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#gearys-c",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#gearys-c",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Geary’s C",
    "text": "Geary’s C\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\nComputing Monte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\nVisualising the Monte Carlo Geary’s C\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-correlogram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-correlogram",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Spatial Correlogram",
    "text": "Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\nCompute Moran’s I correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\nMI_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCompute Geary’s C correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\n\n\n\n\n\nNote\n\n\n\nNote how Geary’s C uses a different method and style from Moran’s I\n\n\n\nGC_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#cluster-and-outlier-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#cluster-and-outlier-analysis",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Cluster and Outlier Analysis",
    "text": "Cluster and Outlier Analysis\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, you will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\nComputing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\nfips <- order(hunan$County)\nlocalMI <- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n\nMapping the local Moran’s I\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\nhunan.localMI <- cbind(hunan,localMI) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\nMapping the local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nMapping local Moran’s I p-values\nThe choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nMapping both local Moran’s I values and p-values\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\nlocalMI.map <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-a-lisa-cluster-map",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-a-lisa-cluster-map",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Creating a LISA Cluster Map",
    "text": "Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\nPlotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci <- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\n\n\nPlotting Moran scatterplot with standardised variable\nFirst we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\nhunan$Z.GDPPC <- scale(hunan$GDPPC) %>% \n  as.vector \n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\nNow, we are ready to plot the Moran scatterplot again by using the code chunk below.\n\nnci2 <- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\nPreparing LISA map classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\nhunan$lag_GDPPC <- lag.listw(rswm_q, hunan$GDPPC)\nDV <- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)   \n\nThis is followed by centering the local Moran’s around the mean.\n\nLM_I <- localMI[,1] - mean(localMI[,1])  \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif <- 0.05\n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV <0 & LM_I>0] <- 1\nquadrant[DV >0 & LM_I<0] <- 2\nquadrant[DV <0 & LM_I<0] <- 3  \nquadrant[DV >0 & LM_I>0] <- 4      \n\nLastly, places non-significant Moran in the category 0.\n\nquadrant[localMI[,5]>signif] <- 0\n\nIn fact, we can combine all the steps into one single code chunk as shown below:\n\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC <- lag.listw(rswm_q, hunan$GDPPC)\nDV <- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I <- localMI[,1]   \nsignif <- 0.05       \nquadrant[DV <0 & LM_I>0] <- 1\nquadrant[DV >0 & LM_I<0] <- 2\nquadrant[DV <0 & LM_I<0] <- 3  \nquadrant[DV >0 & LM_I>0] <- 4    \nquadrant[localMI[,5]>signif] <- 0\n\n\n\nPlotting LISA map\nNow, we can build the LISA map by using the code chunks below.\n\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\ngdppc <- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Hot Spot and Cold Spot Area Analysis",
    "text": "Hot Spot and Cold Spot Area Analysis\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\nGetis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\nDeriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\nDeriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords <- cbind(longitude, latitude)\n\n\n\nDetermine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n#coords <- coordinates(hunan)\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\nComputing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nnb2listw() is used to convert the nb object into spatial weights object.\n\nwm62_lw <- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\nThe output spatial weights object is called wm62_lw.\n\n\n\nComputing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn <- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw <- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-gi-statistics",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-gi-statistics",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "Computing Gi Statistics",
    "text": "Computing Gi Statistics\n\nGi statistics using fixed distance\n\nfips <- order(hunan$County)\ngi.fixed <- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan.gi <- cbind(hunan, as.matrix(gi.fixed)) %>%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nIn fact, the code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\nMapping Gi values with fixed distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc <- qtm(hunan, \"GDPPC\")\n\nGimap <-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\n\n\nGi statistics using adaptive distance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips <- order(hunan$County)\ngi.adaptive <- localG(hunan$GDPPC, knn_lw)\nhunan.gi <- cbind(hunan, as.matrix(gi.adaptive)) %>%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\nMapping Gi values with adaptive distance weights\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc<- qtm(hunan, \"GDPPC\")\n\nGimap <- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-class Exercise 5",
    "section": "",
    "text": "pacman::p_load(sf, tidyverse, sfdep, tmap, plotly, Kendall)\n\n\n\n\n\n\n\nNote\n\n\n\nInstead of using spdep, we are now using sfdep, which is the new version of spdep. sfdep is an sf version of spdep, able to read the sf layer and skipping the conversion of sf objects to ppp objects."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geospatial",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#geospatial",
    "title": "In-class Exercise 5",
    "section": "Geospatial",
    "text": "Geospatial\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\OdeliaPeh\\IS415-GAA-2024\\In-class_Ex\\In-class_Ex05\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\n\nNote\n\n\n\nCheck whether the bounding box is in decimal degrees or not. If it is in decimal degrees it is in geodetic crs and not projected crs.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMake sure that the geometric type of the file is matched by all of the data points!"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#aspatial",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#aspatial",
    "title": "In-class Exercise 5",
    "section": "Aspatial",
    "text": "Aspatial\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nCheck carefully to ensure there is something that can be used to join the data frame to the geospatial data (eg. County) via identical data in columns"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#relational-join",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#relational-join",
    "title": "In-class Exercise 5",
    "section": "Relational join",
    "text": "Relational join\n\nhunan_GDPPC <- left_join(hunan,hunan2012) %>%\n  select(1:4, 7, 15)\n\n\n\n\n\n\n\nNote\n\n\n\nALWAYS KEEP THE GEOMETRY DATA COLUMN\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the type of file that is pointed to by the join will pass its file type down eg. left_join to a sata frame"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1: Geospatial Data Science with R",
    "section": "",
    "text": "Install and load the required packages:\nInstall via Tools > Install Packages… > Type “pacman” into the “Packages” line\n\nIf needed, pacman\n\nInstall via Tools > Install Packages… > Type “pacman” into the “Packages” line\n\ntidyverse\nsf\n\n\npacman::p_load(sf, tidyverse)\n\n\n\n\nThere are three layers of geospatial data that need to be imported into R using st_read() of the sf package:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\n\nmpsz = st_read(dsn = \"data/Geospatial\",\n               layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\OdeliaPeh\\IS415-GAA-2024\\Hands-on_Ex\\Hands-on_Ex01\\data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\ncyclingpath = st_read(dsn = \"data/Geospatial\", \n                         layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\OdeliaPeh\\IS415-GAA-2024\\Hands-on_Ex\\Hands-on_Ex01\\data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\n\npreschool = st_read(\"data/Geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\OdeliaPeh\\IS415-GAA-2024\\Hands-on_Ex\\Hands-on_Ex01\\data\\Geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\nRetrieve information related to the content of a simple feature data frame using:\n\nst_geometry()\nglimpse()\nhead()\n\n\n\nA general way to retrieve and display basic information of feature classes such as type of geometry, the geographic extent of the features and the coordinate system of the data.\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\n\n\n\nBeside the basic feature information, we also would like to learn more about the associated attribute information in the data frame. glimpse() reveals the data type of each field.\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\n\n\n\nSometimes we would like to reveal complete information of a feature object, this is the job of head(). We can also select the number of records that we would like to see using this function.\n\nhead(mpsz, n=5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\n\n\n\nIn geospatial data science, by looking at the feature information is not enough. We are also interested to visualise the geospatial features. This is where the function plot() comes in. Note that plot() is only used for a quick look.\n\n\nplot(mpsz)\n\n\n\n\nTo plot only geometry:\n\nplot(st_geometry(mpsz))\n\n\n\n\nTo plot using a specific attribute:\n\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\n\n\n\n\n\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nThis is an example the coordinate system of mpsz simple feature data frame by using st_crs() of sf package as shown in the code chunk below.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAlthough mpsz data frame is projected in svy21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for svy21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\nNow, let us check the CSR again.\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nLet us take preschool simple feature data frame as an example. The print below reveals that it is in wgs84 coordinate system.\n\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nThis is a scenario that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\nUse the following code snippet to perform the projection transformation.\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)\n\nWhen checking the content of preschool dataframe, we can now see that it has been reprojected to SVY21\n\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\n\n\n\n\nIn practice, it is not unusual that we will come across data such as listing of Inside Airbnb. We call this kind of data aspatial data. This is because it is not a geospatial data but among the data fields, there are two fields that capture the x- and y-coordinates of the data points. listings from Airbnb is one such example.\n\n\nSince listings data set is in csv file format, we will use read_csv() of readr package to import listing.csv as shown the code chunk below. The output R object is called listings and it is a tibble data frame.\n\nlistings <- read_csv(\"data/Aspatial/listings.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() of Base R instead of glimpse() is used to do the job.\n\nlist(listings) \n\n[[1]]\n# A tibble: 3,457 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    <dbl> <chr>       <dbl> <chr>     <chr>               <chr>            <dbl>\n 1  71609 Villa in…  367042 Belinda   East Region         Tampines          1.35\n 2  71896 Home in …  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Home in …  367042 Belinda   East Region         Tampines          1.35\n 4 275343 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Home in …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 Rental u… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 369141 Place to… 1521514 Elizabeth Central Region      Newton            1.31\n# ℹ 3,447 more rows\n# ℹ 11 more variables: longitude <dbl>, room_type <chr>, price <dbl>,\n#   minimum_nights <dbl>, number_of_reviews <dbl>, last_review <date>,\n#   reviews_per_month <dbl>, calculated_host_listings_count <dbl>,\n#   availability_365 <dbl>, number_of_reviews_ltm <dbl>, license <chr>\n\n\nThe output reveals that listing tibble data frame consists of 4252 rows and 16 columns. Two useful fields we are going to use in the next phase are latitude and longitude. Note that they are in decimal degree format. As a best guess, we will assume that the data is in wgs84 Geographic Coordinate System.\n\n\n\nThe code chunk below converts listing data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\nThings to learn from the arguments above:\n\ncoords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io.\n%>% is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\nExamining the content of this newly created simple feature data frame, a new column called geometry has been added into the data frame while the longitude and latitude columns have been dropped from the data frame.\n\n\n\n\nBesides providing functions to handling (i.e. importing, exporting, assigning projection, transforming projection etc) geospatial data, sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\n\n\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution:\nFirstly, st_buffer() is used to compute the 5-meter buffers around cycling paths\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\n\nThis is followed by calculating the area of the buffers as shown in the code chunk below.\n\nbuffer_cycling$AREA <- st_area(buffer_cycling)\n\nLastly, sum() of Base R will be used to derive the total land involved\n\nsum(buffer_cycling$AREA)\n\n1774367 [m^2]\n\n\nMission Accomplished!\n\n\n\n\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\nWARNING: DO NOT MIX UP WITH st_intersection()\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\n\nYou can check the summary statistics of the newly derived PreSch Count field by using summary() as shown in the code chunk below.\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\n\n\nFirstly, the code chunk below uses st_area() of sf package to derive the area of each planning subzone.\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\n\nNext, mutate() of dplyr package is used to compute the density by using the code chunk below.\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\n\n\n\n\nIn practice, many geospatial analytics start with Exploratory Data Analysis. In this section, you will learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown in the code chunk below.\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\nAlthough the syntax is very easy to use however the output is far from meeting publication quality. Furthermore, the function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2 functions will be used.\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school evenly distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\n\nPersonal try:\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`,\n         x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n            fill=\"light blue\") +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")\n\n\n\n\nSolution (cleaner):\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "",
    "text": "In this exercise, we learn how to plot and visualize choropleth maps using an R package called **tmap** package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#installing-and-loading-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#installing-and-loading-r-packages",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "Installing and loading R packages",
    "text": "Installing and loading R packages\nThe packages that are used in this hands on exercise are:\n\ntmap\nreadr for importing delimited text file\ntidyr for tidying data\ndplyr for wrangling data\nsf for handling geospatial data\n\nWe use the following to install and load the packages in Rstudio:\n\npacman::p_load(sf, tmap, tidyverse)\n\n(Note that readr, tidyr and dplyr are part of tidyverse package)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-the-data",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "Importing the data",
    "text": "Importing the data\nTwo data sets are needed to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) in ESRI format from data.gov.sg\nSingapore Residents by Planning Area/Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format from the Department of Statistics, Singapore"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-data-into-r",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "Importing Data into R",
    "text": "Importing Data into R\n\nGeospatial data\nWe will use st_read to import the geospatial data into R.\n\n#|eval: false\nmpsz <- st_read(dsn = \"data/Geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\OdeliaPeh\\IS415-GAA-2024\\Hands-on_Ex\\Hands-on_Ex02\\data\\Geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\nAspatial data\nWe will use read_csv to import the aspatial data into R.\n\n#|eval: false\npopdata <- read_csv(\"data/Aspatial/respopagesextod2011to2020.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-preparation",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "Data preparation",
    "text": "Data preparation\nFor the map that will be made in this hands-on exercise, we are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\nThese columns represent the following:\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\nData wrangling\nIn this section, the new functions used are:\n\npivot_wider() of tidyr package, which is used to pivot a data frame from a long format to a wide format\nmutate(), filter(), group_by(), select() and summarise() of dplyr package\n\nmutate() creates new columns that are functions of existing variables. It can also modify (if the name is the same as an existing column) and delete columns\nfilter(): allows you to select a subset of rows in a data frame\ngroup_by(): it takes a data frame and one or more variables to group by\nselect(): subsets columns by position, name, function of name, or other property\nsummarise(): collapses data into a single row\n\n\n\nAspatial data preparation\n\n#|eval: false\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\nJoining attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other hand the SUBZONE_N and PLN_AREA_N are in uppercase.\nWithout transformation to ensure that the fields are the same, we will be unable to perform the georelational join.\n\n#|eval: false\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\n#|eval: false\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\nThe next code chunk is to save the new table in the readr(rds) format.\n\n#|eval: false\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\n\n\n\nImportant\n\n\n\nHaving saved the new table as an rds, there is no need to go through the reformatting of the data again. Instead, we can simply call the rds file directly. For more information, refer to In-class Exercise 2.\n\n\n\nmpsz_pop2020 <- read_rds(\"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "Choropleth Mapping Geospatial Data Using tmap",
    "text": "Choropleth Mapping Geospatial Data Using tmap\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm()\nPlotting highly customisable thematic map by using tmap elements.\n\n\nPlotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used (example below)\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\nCreating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\nExploration of tmap functions for plotting a map\n\nDrawing of base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\nDrawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. We learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey\n\n\n\nDrawing a choropleth map using tm_fill() and tm_border()\ntm_polygons() is actually a wrapper of tm_fill() and tm_border().\ntm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nNote that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”\n\n\n\n\nData classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total of ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\nPlotting with built-in classification methods\nThe code chunk below shows a quantile data classification that uses 5 classes. Jenks is a data classification type that uses natural break points in the data set.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nThis next code chunk below uses a equal data classification\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method. Different data classification types are better for different types of maps.\n\nDIY: different number of classes\nFor this DIY, I will be using the equal data classification type.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\nPlotting choropleth map with custom break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\nColour Schemes\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nSee how the colour scheme for each bin has\n\n\nMap Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\nLegend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNote that legend.hist and related functions are what creates the histogram shown on the map above.\n\n\nMap styles\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\nCartographic furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\nDrawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments\nby defining a group-by variable in tm_facets()\nby creating multiple stand-alone maps with tmap_arrange().\n\n\nAssigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill(). In the below code chunk, multiple columns are defined by tm_fill(c(\"YOUNG\",\"AGED\")).\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nThe use of c(X, Y) can also be used to create multiple small choropleth maps by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\nBy defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNote how tm_facets() creates multiple small choropleth maps that are grouped by the variable that is inputted.\n#####By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\nMappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection function to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#extra-diy-tmap_modeview",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#extra-diy-tmap_modeview",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualisation with R",
    "section": "Extra DIY: tmap_mode(view)",
    "text": "Extra DIY: tmap_mode(view)\n\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\nqtm(mpsz_pop2020, fill = \"DEPENDENCY\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "",
    "text": "Spatial Point Pattern Analysis is the evaluation of the pattern or distribution, of a set of points on a surface. Using appropriate functions of spatstat, we can discover the spatial point processes of childecare centres in Singapore.\nThe specific questions we would like to answer in this exercise are as follows:\n\nare the childcare centres in Singapore randomly distributed throughout the country?\nif the answer is not, then the next logical question is where are the locations with higher concentration of childcare centres?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#assigning-the-correct-epsg-code",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#assigning-the-correct-epsg-code",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Assigning the correct EPSG code",
    "text": "Assigning the correct EPSG code\nNow, we need to check mpsz_sf to ensure that it is using the correct EPSG code.\n\nst_crs(mpsz_sf)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAs can be seen, despite being projected in SVY21, the EPSG code is incorrect. We can fix this using st_transform.\n\nmpsz3414_sf <- st_transform(mpsz_sf, 3414)\n\nNow we can check the CRS again. This time, it should be shown as 3414.\n\nst_crs(mpsz3414_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\nplot(mpsz3414_sf)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#diy-creating-coastaloutline",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#diy-creating-coastaloutline",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "DIY: Creating CoastalOutline",
    "text": "DIY: Creating CoastalOutline\nWe need to derive the CoastalOutline from mpsz_3414. Using examples found in the documents of sf, I have come up with the following steps:\nFirst, we need to check the type of data we have.\n\nst_geometry(mpsz3414_sf)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\nFrom the above, we can see that mpsz3414_sf is a geometry set. In order to use get a clean coastal outline for Singapore, we will need to convert these into simple geometric features using st_as_sf. This is because otherwise, when we use st_union, the subzone boundaries will still be present as shown below.\n\nCoastalOutline1 <- st_combine(mpsz3414_sf)\nplot(st_union(CoastalOutline1))\n\n\n\n\nInstead, we should first use st_as_sf to transform the data into a simple feature collection.\n\nCoastalOutline <- st_as_sf(mpsz3414_sf)\nCoastalOutline\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nNow, we should be able to use st_union in conjunction with plot to see a Singapore as just the Coastal Outlines!\n\nplot(st_union(CoastalOutline))\n\n\n\n\n\n\n\n\n\n\nMain lessons from CoastalOutline\n\n\n\n\nmpsz3414_sf does not have a simple features geometry column (from an error message while attempting to use st_read(system.file()), which prevents reading the shp file directly\nTo transform a geometry set into a simple feature set, we use st_as_sf\nst_union is using while plotting to reveal the CoastalOutline"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#checking-the-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#checking-the-data",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Checking the data",
    "text": "Checking the data\nIt is important that we ensure that the data is accurate. To do that here, since all of our data is geospatial in nature, we can simply plot a map.\n\n\n\n\n\n\nTip\n\n\n\nBecause childcare_sf is a collection of point data, we use tm_dots instead of tm_polygons.\n\n\n\ntm_shape(CoastalOutline) + \n  tm_polygons() + \n  tm_shape(mpsz3414_sf) + \n  tm_polygons() + \n  tm_shape(childcare_sf) + \n  tm_dots()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-sf-data-frames-to-sps-spatial-class",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-sf-data-frames-to-sps-spatial-class",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Converting sf data frames to sp’s Spatial* class",
    "text": "Converting sf data frames to sp’s Spatial* class\nThere are many geospatial analysis packages require the input geospatial data in sp’s Spatial* classes. Hence, we will use as_Spatial to convert the three geospatial data from a simple feature dataframe to sp’s Spatial* class.\n\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz3414_sf)\nsg <- as_Spatial(st_union(CoastalOutline))\n\nBelow, we can see what the data looks like now\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1925 \nextent      : 11810.03, 45404.24, 25596.33, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Description \nmin values  :   kml_1, <center><table><tr><th colspan='2' align='center'><em>Attributes</em></th></tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSBLOCKHOUSENUMBER</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSBUILDINGNAME</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSPOSTALCODE</th> <td>100044</td> </tr><tr bgcolor=\"\"> <th>ADDRESSSTREETNAME</th> <td>44, TELOK BLANGAH DRIVE, #01 - 19/51, SINGAPORE 100044</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSTYPE</th> <td></td> </tr><tr bgcolor=\"\"> <th>DESCRIPTION</th> <td>Child Care Services</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>HYPERLINK</th> <td></td> </tr><tr bgcolor=\"\"> <th>LANDXADDRESSPOINT</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>LANDYADDRESSPOINT</th> <td></td> </tr><tr bgcolor=\"\"> <th>NAME</th> <td>PCF SPARKLETOTS PRESCHOOL @ TELOK BLANGAH BLK 44 (CC)</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>PHOTOURL</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSFLOORNUMBER</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>INC_CRC</th> <td>349C54F201805938</td> </tr><tr bgcolor=\"\"> <th>FMEL_UPD_D</th> <td>20211201093837</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSUNITNUMBER</th> <td></td> </tr></table></center> \nmax values  : kml_999,                                            <center><table><tr><th colspan='2' align='center'><em>Attributes</em></th></tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSBLOCKHOUSENUMBER</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSBUILDINGNAME</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSPOSTALCODE</th> <td>99982</td> </tr><tr bgcolor=\"\"> <th>ADDRESSSTREETNAME</th> <td>35, ALLANBROOKE ROAD, SINGAPORE 099982</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSTYPE</th> <td></td> </tr><tr bgcolor=\"\"> <th>DESCRIPTION</th> <td>Child Care Services</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>HYPERLINK</th> <td></td> </tr><tr bgcolor=\"\"> <th>LANDXADDRESSPOINT</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>LANDYADDRESSPOINT</th> <td></td> </tr><tr bgcolor=\"\"> <th>NAME</th> <td>ISLANDER PRE-SCHOOL PTE LTD</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>PHOTOURL</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSFLOORNUMBER</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>INC_CRC</th> <td>4F63ACF93EFABE7F</td> </tr><tr bgcolor=\"\"> <th>FMEL_UPD_D</th> <td>20211201093837</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSUNITNUMBER</th> <td></td> </tr></table></center> \n\n\n\nmpsz\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\n\nsg\n\nclass       : SpatialPolygons \nfeatures    : 1 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\nNote how the class has changed entirely to SpatialPolygonsDataFrame"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-spatial-class-to-generic-sp-format",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-spatial-class-to-generic-sp-format",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Converting the Spatial* class to generic sp format",
    "text": "Converting the Spatial* class to generic sp format\nspatstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\nThis code chunk converts the Spatial* class into generic sp objects.\n\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\n\n\nchildcare_sp\n\nclass       : SpatialPoints \nfeatures    : 1925 \nextent      : 11810.03, 45404.24, 25596.33, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 1 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\n\n\n\n\n\nNote\n\n\n\nIt appears that the main difference that can be observed is that a sp object does not include a dataframe of data (such as name and other aspatial data). It is entirely made up of geospatial data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Converting the generic sp format into spatstat’s ppp format",
    "text": "Converting the generic sp format into spatstat’s ppp format\nNext, we use the as.ppp() function to convert the spatial data into spatstat’s ppp object. (Finally!)\n\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nPlanar point pattern: 1925 points\nwindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n\n\nWe can now plot childcare_ppp to view the difference. (It’s actually circular points instead of crosses now! And childcare_ppp includes a header as well)\n\nplot(childcare_ppp)\n\n\n\nplot(childcare)\n\n\n\n\nWe can also look at the summary statistics for the newly created ppp object.\n\nsummary(childcare_ppp)\n\nPlanar point pattern:  1925 points\nAverage intensity 2.417323e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units\n\n\nNotice the warning message about duplicates. In spatial point patterns analysis an issue of significance is the presence of duplicates. The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#handling-duplicated-points",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#handling-duplicated-points",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Handling duplicated points",
    "text": "Handling duplicated points\nWe can check the duplication in a ppp object by using the code chunk below.\n\nany(duplicated(childcare_ppp))\n\n[1] TRUE\n\n\nTo count the number of co-indicence point, we will use the multiplicity() function as shown in the code chunk below.\n\n\n\n\n\n\nWarning\n\n\n\nI am choosing not to run this because it returns a MASSIVE amount of data.\n\n\n\nmultiplicity(childcare_ppp)\n\nIf we want to know how many locations have more than one point event, we can use the code chunk below.\n\nsum(multiplicity(childcare_ppp) > 1)\n\n[1] 338\n\n\nTo view the locations of these duplicate point events, we can plot childcare data by using the code chunk below.\n\ntmap_mode('view')\ntm_shape(childcare) +\n  tm_dots(alpha=0.4, \n          size=0.05)\n\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\n\n\n\n\nPossible method of identification\n\n\n\nFrom observation, it is possible that the way we identify duplicate locations is by how dark/opaque the point identifier is, which indicates multiple points overlaying each other.\n\n\nNotice that at the interactive mode, tmap is using leaflet for R API. The advantage of this interactive pin map is it allows us to navigate and zoom around the map freely. We can also query the information of each simple feature (i.e. the point) by clicking of them. Last but not least, you can also change the background of the internet map layer. Currently, three internet map layers are provided. They are: ESRI.WorldGrayCanvas, OpenStreetMap, and ESRI.WorldTopoMap. The default is ESRI.WorldGrayCanvas.\n\n\n\n\n\n\nImportant\n\n\n\nReminder: Always remember to switch back to plot mode after the interactive map. This is because, each interactive mode will consume a connection. You should also avoid displaying excessive numbers of interactive maps (i.e. not more than 10) in one RMarkdown document when publish on Netlify.\n\n\nThere are three ways to overcome this problem. The easiest way is to delete the duplicates. But, tdoing this will mean that some useful point events will be lost.\nThe second solution is use jittering, which will add a small perturbation to the duplicate points so that they do not occupy the exact same space.\nThe third solution is to make each point “unique” and then attach the duplicates of the points to the patterns as marks, as attributes of the points. Then you would need analytical techniques that take into account these marks.\nThe code chunk below implements the jittering approach.\n\nchildcare_ppp_jit <- rjitter(childcare_ppp, \n                             retry=TRUE, \n                             nsim=1, \n                             drop=TRUE)\n\nNow, we can check again whether there are any duplicate data points in childcare_ppp_jit\n\nany(duplicated(childcare_ppp_jit))\n\n[1] FALSE"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#creating-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#creating-owin-object",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Creating owin object",
    "text": "Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Singapore boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to covert sg SpatialPolygon object into owin object of spatstat.\n\nsg_owin <- as(sg_sp, \"owin\")\n\nWe can see the output by plotting it.\n\nplot(sg_owin)\n\n\n\n\nAnd we can check it using summary() as well\n\nsummary(sg_owin)\n\nWindow: polygonal boundary\n80 separate polygons (35 holes)\n                  vertices         area relative.area\npolygon 1                4  9.47108e+01      1.21e-07\npolygon 2               37  1.29481e+04      1.66e-05\npolygon 3               30  4.28933e+03      5.49e-06\npolygon 4              145  9.61782e+05      1.23e-03\npolygon 5              227  1.10308e+06      1.41e-03\npolygon 6               19  3.09221e+04      3.95e-05\npolygon 7               10  6.60195e+03      8.44e-06\npolygon 8              234  2.08755e+06      2.67e-03\npolygon 9               22  6.74651e+03      8.63e-06\npolygon 10              71  5.63061e+03      7.20e-06\npolygon 11              10  1.99717e+02      2.55e-07\npolygon 12           14663  6.97996e+08      8.93e-01\npolygon 13 (hole)        3 -2.05920e-03     -2.63e-12\npolygon 14 (hole)        3 -2.89050e-05     -3.70e-14\npolygon 15 (hole)        3 -2.83151e-01     -3.62e-10\npolygon 16 (hole)        3 -3.99521e-02     -5.11e-11\npolygon 17 (hole)        3 -4.95057e-02     -6.33e-11\npolygon 18 (hole)        3 -3.65501e-03     -4.67e-12\npolygon 19 (hole)        4 -2.05611e-02     -2.63e-11\npolygon 20 (hole)        3 -1.68316e-04     -2.15e-13\npolygon 21 (hole)       26 -1.25665e+03     -1.61e-06\npolygon 22 (hole)        3 -2.18000e-06     -2.79e-15\npolygon 23 (hole)        3 -6.62377e-01     -8.47e-10\npolygon 24 (hole)        3 -2.09065e-03     -2.67e-12\npolygon 25 (hole)       36 -7.79904e+03     -9.97e-06\npolygon 26 (hole)        3 -8.83647e-03     -1.13e-11\npolygon 27 (hole)        3 -2.21090e+00     -2.83e-09\npolygon 28 (hole)       40 -6.00381e+03     -7.68e-06\npolygon 29 (hole)        7 -1.40545e-01     -1.80e-10\npolygon 30 (hole)       20 -4.39069e+00     -5.62e-09\npolygon 31 (hole)       28 -1.99862e+01     -2.56e-08\npolygon 32 (hole)       48 -1.38338e+02     -1.77e-07\npolygon 33 (hole)      351 -1.21433e+03     -1.55e-06\npolygon 34 (hole)       12 -8.36709e+01     -1.07e-07\npolygon 35 (hole)      317 -5.11280e+04     -6.54e-05\npolygon 36 (hole)       36 -4.01660e+04     -5.14e-05\npolygon 37              30  2.80002e+04      3.58e-05\npolygon 38              27  1.50315e+04      1.92e-05\npolygon 39              15  4.03300e+04      5.16e-05\npolygon 40            1045  4.44510e+06      5.68e-03\npolygon 41 (hole)       13 -3.91907e+02     -5.01e-07\npolygon 42              47  3.82087e+04      4.89e-05\npolygon 43              65  8.42861e+04      1.08e-04\npolygon 44             478  2.06120e+06      2.64e-03\npolygon 45             266  1.50631e+06      1.93e-03\npolygon 46             234  4.72886e+05      6.05e-04\npolygon 47              14  5.86546e+03      7.50e-06\npolygon 48              83  5.28920e+03      6.76e-06\npolygon 49              75  1.73526e+04      2.22e-05\npolygon 50             148  3.10395e+03      3.97e-06\npolygon 51             142  3.22293e+03      4.12e-06\npolygon 52              45  2.51218e+03      3.21e-06\npolygon 53              40  1.38607e+04      1.77e-05\npolygon 54              10  4.90942e+02      6.28e-07\npolygon 55              95  5.96187e+04      7.62e-05\npolygon 56 (hole)        4 -1.86410e-02     -2.38e-11\npolygon 57              64  3.43149e+04      4.39e-05\npolygon 58 (hole)        3 -1.98390e-03     -2.54e-12\npolygon 59 (hole)        3 -5.55856e-03     -7.11e-12\npolygon 60 (hole)        3 -5.12482e-03     -6.55e-12\npolygon 61 (hole)        3 -1.96410e-03     -2.51e-12\npolygon 62 (hole)        4 -1.13774e-02     -1.46e-11\npolygon 63             155  2.67502e+05      3.42e-04\npolygon 64             106  3.04104e+03      3.89e-06\npolygon 65            1027  1.27782e+06      1.63e-03\npolygon 66 (hole)        3 -3.23310e-04     -4.13e-13\npolygon 67 (hole)        3 -1.16959e-03     -1.50e-12\npolygon 68 (hole)        3 -1.46474e-03     -1.87e-12\npolygon 69             211  4.70521e+05      6.02e-04\npolygon 70               4  2.69313e+02      3.44e-07\npolygon 71             132  9.53357e+04      1.22e-04\npolygon 72               6  4.50259e+02      5.76e-07\npolygon 73             285  1.61128e+06      2.06e-03\npolygon 74              91  1.49663e+04      1.91e-05\npolygon 75              71  8.18750e+03      1.05e-05\npolygon 76             668  5.40368e+07      6.91e-02\npolygon 77              77  3.29939e+05      4.22e-04\npolygon 78             711  1.28815e+07      1.65e-02\npolygon 79 (hole)        3 -3.41405e-01     -4.37e-10\npolygon 80              44  2.26577e+03      2.90e-06\nenclosing rectangle: [2667.54, 56396.44] x [15748.72, 50256.33] units\n                     (53730 x 34510 units)\nWindow area = 781945000 square units\nFraction of frame area: 0.422"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#combining-point-events-object-and-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#combining-point-events-object-and-owin-object",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Combining point events object and owin object",
    "text": "Combining point events object and owin object\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\n\n\n\n\n\nSlight alteration\n\n\n\nI noticed that the exercise uses childcare_ppp instead of childcare_ppp_jit. Since the main difference appears to be cleaner data, I will be using childcare_ppp_jit\n\n\n\nchildcareSG_ppp = childcare_ppp_jit[sg_owin]\n\n\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#kernel-density-estimation",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#kernel-density-estimation",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Kernel Density Estimation",
    "text": "Kernel Density Estimation\n\nComputing kernel density estimation using automatic bandwidth selection method\nThe code chunk below computes a kernel density by using the following configurations of density() of spatstat:\n\nbw.diggle() automatic bandwidth (radius) selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\nThe smoothing kernel used is gaussian, which is the default. Other smoothing methods are: “epanechnikov”, “quartic” or “disc”.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\nkde_childcareSG_bw <- density(childcareSG_ppp,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                            kernel=\"gaussian\") \n\nThe plot() function of Base R is then used to display the kernel density derived.\n\nplot(kde_childcareSG_bw)\n\n\n\n\nThe density values of the output range from 0 to 0.000035 which is way too small to comprehend. This is because the default unit of measurement of svy21 is in meters. As a result, the density values computed is in “number of points per square meter”.\nBefore we move on to next section, it is good to know that you can retrieve the bandwidth used to compute the kde layer by using the code chunk below.\n\nbw <- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n312.3243 \n\n\n\n\nRescalling KDE values\nIn the code chunk below, rescale() is used to covert the unit of measurement from meter to kilometer.\n\nchildcareSG_ppp.km <- rescale(childcareSG_ppp, 1000, \"km\")\n\nWith this, we can rerun density() and plot the output map\n\nkde_childcareSG.bw <- density(childcareSG_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG.bw)\n\n\n\n\nNow, despite the map not changing at all, we can see that the scale of the legend is a lot more readable now!\n\n\nWorking with different automatic badwidth methods\nBeside bw.diggle(), there are three other spatstat functions can be used to determine the bandwidth, they are: bw.CvL(), bw.scott(), and bw.ppl().\nLet us take a look at the bandwidth return by these automatic bandwidth calculation methods by using the code chunk below.\n\n bw.CvL(childcareSG_ppp.km)\n\n   sigma \n3.928273 \n\n bw.scott(childcareSG_ppp.km)\n\n sigma.x  sigma.y \n2.160569 1.395330 \n\n bw.ppl(childcareSG_ppp.km)\n\n   sigma \n0.277574 \n\n bw.diggle(childcareSG_ppp.km)\n\n    sigma \n0.3123243 \n\n\nBaddeley et. (2016) suggested the use of the bw.ppl() algorithm because in ther experience it tends to produce the more appropriate values when the pattern consists predominantly of tight clusters. But they also insist that if the purpose of once study is to detect a single tight cluster in the midst of random noise then the bw.diggle() method seems to work best.\nThe code chunk beow will be used to compare the output of using bw.diggle and bw.ppl methods.\n\nkde_childcareSG.ppl <- density(childcareSG_ppp.km, \n                               sigma=bw.ppl, \n                               edge=TRUE,\n                               kernel=\"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\nWorking with different kernel methods\nBy default, the kernel method used in density.ppp() is gaussian. But there are three other options, namely: Epanechnikov, Quartic and Dics.\nThe code chunk below will be used to compute three more kernel density estimations by using these three kernel function.\n\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"gaussian\"), \n     main=\"Gaussian\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"epanechnikov\"), \n     main=\"Epanechnikov\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"quartic\"), \n     main=\"Quartic\")\nplot(density(childcareSG_ppp.km, \n             sigma=bw.ppl, \n             edge=TRUE, \n             kernel=\"disc\"), \n     main=\"Disc\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#fixed-and-adaptive-kde",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#fixed-and-adaptive-kde",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Fixed and Adaptive KDE",
    "text": "Fixed and Adaptive KDE\n\nComputing KDE by using fixed bandwidth\nNext, you will compute a KDE layer by defining a bandwidth of 600 meter. Notice that in the code chunk below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\n\nkde_childcareSG_600 <- density(childcareSG_ppp.km, sigma=0.6, edge=TRUE, kernel=\"gaussian\")\nplot(kde_childcareSG_600)\n\n\n\n\n\n\nComputing KDE using an adaptive bandwidth\nFixed bandwidth method is very sensitive to highly skewed distribution of spatial point patterns over geographical units for example urban versus rural. One way to overcome this problem is by using an adaptive bandwidth instead.\nIn this section, you will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\nkde_childcareSG_adaptive <- adaptive.density(childcareSG_ppp.km, method=\"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\nConverting KDE output into grid object\nThe result is the same, we just convert it so that it is suitable for mapping purposes\n\ngridded_kde_childcareSG_bw <- as.SpatialGridDataFrame.im(kde_childcareSG.bw)\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n\n\nConversion to raster\nNext, we will convert the gridded kernal density objects into RasterLayer object by using raster() of raster package.\n\nkde_childcareSG_bw_raster <- raster(gridded_kde_childcareSG_bw)\n\nLet us take a look at the properties of kde_childcareSG_bw_raster RasterLayer.\n\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.419757, 0.2695907  (x, y)\nextent     : 2.667538, 56.39644, 15.74872, 50.25633  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : v \nvalues     : -1.21856e-14, 36.12115  (min, max)\n\n\nNote how there is no assigned crs, which means that we need to assign the raster data a crs on our own.\n\nprojection(kde_childcareSG_bw_raster) <- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.419757, 0.2695907  (x, y)\nextent     : 2.667538, 56.39644, 15.74872, 50.25633  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : v \nvalues     : -1.21856e-14, 36.12115  (min, max)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#visualising-the-output-in-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#visualising-the-output-in-tmap",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Visualising the output in tmap",
    "text": "Visualising the output in tmap\nFinally, we will display the raster in cartographic quality map using tmap package.\n\ntm_shape(kde_childcareSG_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\nNotice that the raster values are encoded explicitly onto the raster pixel using the values in “v”” field."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#comparing-spatial-point-patterns-using-kde",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#comparing-spatial-point-patterns-using-kde",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Comparing Spatial Point Patterns using KDE",
    "text": "Comparing Spatial Point Patterns using KDE\nFor this section, we will be comparing spatial point patterns between different planning areas.\n\nData wrangling: study areas\nFirst, we need the extract the target study areas and plot them\n\npg = mpsz[mpsz@data$PLN_AREA_N == \"PUNGGOL\",]\ntm = mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\",]\nck = mpsz[mpsz@data$PLN_AREA_N == \"CHOA CHU KANG\",]\njw = mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\",]\n\n\npar(mfrow=c(2,2))\nplot(pg, main = \"Ponggol\")\nplot(tm, main = \"Tampines\")\nplot(ck, main = \"Choa Chu Kang\")\nplot(jw, main = \"Jurong West\")\n\n\n\n\nNext, we will convert these SpatialPolygonsDataFrame layers into generic spatialpolygons layers.\n\npg_sp = as(pg, \"SpatialPolygons\")\ntm_sp = as(tm, \"SpatialPolygons\")\nck_sp = as(ck, \"SpatialPolygons\")\njw_sp = as(jw, \"SpatialPolygons\")\n\nNow, we will convert these SpatialPolygons objects into owin objects that is required by spatstat.\n\npg_owin = as(pg_sp, \"owin\")\ntm_owin = as(tm_sp, \"owin\")\nck_owin = as(ck_sp, \"owin\")\njw_owin = as(jw_sp, \"owin\")\n\nThen, we combine to the childcare_ppp_jit layer to extract childcare that is within the specific region to do our analysis later on.\n\nchildcare_pg_ppp = childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp = childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp = childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp = childcare_ppp_jit[jw_owin]\n\nNext, rescale() function is used to trasnform the unit of measurement from metre to kilometre\n\nchildcare_pg_ppp.km = rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km = rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km = rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km = rescale(childcare_jw_ppp, 1000, \"km\")\n\nThe code chunk below is used to plot these four study areas and the locations of the childcare centres.\n\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main=\"Punggol\")\nplot(childcare_tm_ppp.km, main=\"Tampines\")\nplot(childcare_ck_ppp.km, main=\"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main=\"Jurong West\")\n\n\n\n\nNow, we can compute the KDE of each planning areas. bw.diggle method is used to derive the bandwidth of each.\n\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tempines\")\nplot(density(childcare_ck_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=bw.diggle, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\n\n\n\n\nFirst, let us compute the fixed bandwidth. In this exercise, it would be set at 250m.\n\npar(mfrow=c(2,2))\nplot(density(childcare_ck_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Chou Chu Kang\")\nplot(density(childcare_jw_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"JUrong West\")\nplot(density(childcare_pg_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Punggol\")\nplot(density(childcare_tm_ppp.km, \n             sigma=0.25, \n             edge=TRUE, \n             kernel=\"gaussian\"),\n     main=\"Tampines\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#nearest-neighbour-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#nearest-neighbour-analysis",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Nearest Neighbour Analysis",
    "text": "Nearest Neighbour Analysis\nn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\nTesting spatial point patterns using Clark and Evans Test\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.50944, p-value < 2.2e-16\nalternative hypothesis: clustered (R < 1)\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause p-value < 0.05, we reject H0\n\n\n\n\nClark and Evans Test: Choa Chu Kang planning area\n\nclarkevans.test(childcare_ck_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.88084, p-value = 0.04988\nalternative hypothesis: two-sided\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause p-value > 0.05, we cannot reject H0\n\n\n\n\nClark and Evans Test: Tampines planning area\n\nclarkevans.test(childcare_tm_ppp,\n                correction=\"none\",\n                clipregion=NULL,\n                alternative=c(\"two.sided\"),\n                nsim=999)\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.69764, p-value = 3.931e-10\nalternative hypothesis: two-sided\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause p-value < 0.05, we reject H0"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-g-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-g-function",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Analysing Spatial Point Process Using G-Function",
    "text": "Analysing Spatial Point Process Using G-Function\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event. In this section, you will learn how to compute G-function estimation by using Gest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\nChoa Chu Kang Planning Area\n\nComputing G-function estimation\nThe code chunk below is used to compute G-function using Gest() of spatat package.\n\nG_CK = Gest(childcare_ck_ppp, correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-function (simulation testing)\n\nG_CK.csr <- envelope(childcare_ck_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(G_CK.csr)\n\n\n\n\n\n\n\nTampines Planning Area\n\nComputing G-function estimation\n\nG_tm = Gest(childcare_tm_ppp, correction = \"best\")\nplot(G_tm)\n\n\n\n\n\n\nPerforming Complete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nG_tm.csr <- envelope(childcare_tm_ppp, Gest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(G_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-f-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-f-function",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Analysing Spatial Point Process Using F-Function",
    "text": "Analysing Spatial Point Process Using F-Function\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, you will learn how to compute F-function estimation by using Fest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\nChoa Chu Kang Area\n\nComputing F-function estimation\nThe code chunk below is used to compute F-function using Fest() of spatat package.\n\nF_CK = Fest(childcare_ck_ppp)\nplot(F_CK)\n\n\n\n\n\n\nComplete Spatial Randomness test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with F-fucntion\n\nF_CK.csr <- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_CK.csr)\n\n\n\n\n\n\n\nTampines Area\n\nComputing F-function estimation\n\nF_tm = Fest(childcare_tm_ppp, correction = \"best\")\nplot(F_tm)\n\n\n\n\n\n\nComplete Spatial Randomness test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected is p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nF_tm.csr <- envelope(childcare_tm_ppp, Fest, correction = \"all\", nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\nplot(F_tm.csr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-k-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-k-function",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Analysing Spatial Point Process Using K-Function",
    "text": "Analysing Spatial Point Process Using K-Function\nK-function measures the number of events found up to a given distance of any particular event. In this section, you will learn how to compute K-function estimates by using Kest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\nChoa Chu Kang planning area\n\nComputing K-function estimate\n\nK_ck = Kest(childcare_ck_ppp, correction = \"Ripley\")\nplot(K_ck, . -r ~ r, ylab= \"K(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\nComplete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nK_ck.csr <- envelope(childcare_ck_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\nTampines planning area\n\nComputing K-function estimate\n\nK_tm = Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_tm, . -r ~ r, \n     ylab= \"K(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\nComplete Spatial Randomness Test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nK_tm.csr <- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(K_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"K(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-l-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analysing-spatial-point-process-using-l-function",
    "title": "Hands-on Exercise 3: Spatial Point Patterns Analysis Methods",
    "section": "Analysing Spatial Point Process Using L-Function",
    "text": "Analysing Spatial Point Process Using L-Function\nIn this section, you will learn how to compute L-function estimation by using Lest() of spatstat package. You will also learn how to perform monta carlo simulation test using envelope() of spatstat package.\n\nChoa Chu Kang Planning Area\n\nComputing L Fucntion estimation\n\nL_ck = Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_ck, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\nComplete Spatial Randomness test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\nThe code chunk below is used to perform the hypothesis testing.\n\nL_ck.csr <- envelope(childcare_ck_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(L_ck.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\n\n\nTampines planning area\n\nComputing L-function estimate\n\nL_tm = Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_tm, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\", \n     xlim=c(0,1000))\n\n\n\n\n\n\nComplete Spatial Randomness testing\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis and test are as follows:\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nThe code chunk below will be used to perform the hypothesis testing.\n\nL_tm.csr <- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal=TRUE)\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\nplot(L_tm.csr, . - r ~ r, \n     xlab=\"d\", ylab=\"L(d)-r\", xlim=c(0,500))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#compute-global-morans-i-permutation-test",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#compute-global-morans-i-permutation-test",
    "title": "In-class Exercise 5",
    "section": "Compute global Moran’s I permutation test",
    "text": "Compute global Moran’s I permutation test\nTo confirm statistical significance, we need to conduct Moral testing (receive p value. to reect or accept the hypothesis)\nInstead, we want the permutation test, which involves simulation. Most of the time, we only need to calculate the Global Moran I Permutation so we have more iterations to backup the analysis and confirm the representativeness of the dataset.\nWe can also set.seed() before performing the simulation to ensure the computation is reproducible.\n\nset.seed(1234)\n\n\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\n\nReject null hypothesis, hence you can INFER that there is clustering (also because the statistic is more than 0). This method is most convincing overall.\nNow, we want to know if it is a hot/cold spot, and whether or not it is an outlier in the data."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#local-gi-statistics",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#local-gi-statistics",
    "title": "In-class Exercise 5",
    "section": "Local Gi* statistics",
    "text": "Local Gi* statistics\nSpecially calibrated to detect hot and cold spots. Gi statistic is also a distance based method.\nWe provide weights, and using inverse distance there is more weight given to neighbours that are closer based on distance.\n\nwm_idw <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\n\n\n\n\n\nNote\n\n\n\nThe scale is there to convert the distance. 1 allows us to convert it to km"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#creating-time-series-cube",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html#creating-time-series-cube",
    "title": "In-class Exercise 5",
    "section": "Creating Time Series Cube",
    "text": "Creating Time Series Cube\nUsing the function spacetime to create a cube where the height represents the time, and each cube inside represents a geographic area.\n\nGDPPC_st <- spacetime(GDPPC, hunan,\n                      .loc_col = \"County\",\n                      .time_col = \"Year\")\n\n\nis_spacetime_cube(GDPPC_st)\n\n[1] TRUE\n\n\n\nDeriving spatial weights\n\nGDPPC_nb <- GDPPC_st %>%\n  activate(\"geometry\")  %>% #only pulls out the geometry layer\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n\nNote that this dataset now has neighbors and weights for each time-slice.\n\nhead(GDPPC_nb)\n\n# A tibble: 6 × 5\n   Year County  GDPPC nb        wt       \n  <dbl> <chr>   <dbl> <list>    <list>   \n1  2005 Anxiang  8184 <int [6]> <dbl [6]>\n2  2005 Hanshou  6560 <int [6]> <dbl [6]>\n3  2005 Jinshi   9956 <int [5]> <dbl [5]>\n4  2005 Li       8394 <int [5]> <dbl [5]>\n5  2005 Linli    8850 <int [5]> <dbl [5]>\n6  2005 Shimen   9244 <int [6]> <dbl [6]>\n\n\n\ngi_stars <- GDPPC_nb %>% \n  group_by(Year) %>% \n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %>% \n  tidyr::unnest(gi_star)"
  },
  {
    "objectID": "Take-home_Ex/Take-Home_Ex02/data/geospatial/TAINAN_VILLAGE.html",
    "href": "Take-home_Ex/Take-Home_Ex02/data/geospatial/TAINAN_VILLAGE.html",
    "title": "IS415-GAA-2024",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>"
  },
  {
    "objectID": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html",
    "href": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "For this geospatial task, we have been tasked to discover:\n\nif the distribution of dengue fever outbreak at Tainan City, Taiwan are independent from space and space and time.\nIf the outbreak is indeed spatial and spatio-temporal dependent, then, you would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas."
  },
  {
    "objectID": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#geospatial-data",
    "href": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#geospatial-data",
    "title": "Take-home Exercise 2",
    "section": "Geospatial data",
    "text": "Geospatial data\nFor geospatial data, using st_read(), I will import the geospatial data file TAINAN_VILLAGE, which contains the geospatial data of village boundary of Taiwan.\n\ntaiwan <- st_read(dsn = \"data/geospatial\", \n                layer = \"TAINAN_VILLAGE\")\n\n\n\n\n\n\n\nNote\n\n\n\nThe data is in Taiwan Geographic Coordinate System, TWD97. This should be kept unchanged to ensure that the data is as accurate as possible to the collected data.\n\n\nNow the map can be plotted so that we can see what we are working with, grouped by TOWNID column.\n\nplot(taiwan[\"TOWNID\"])\n\nThe geospatial data here is not limited to only the city of Tainan. To filter out Tainan City for the use of this exercise, I will filter for the following counties:\n\nD01\nD02\nD04\nD06\nD07\nD08\nD32\nD39\n\nThis will make the later parts of data cleaning faster due to having less to process.\nFrom inspecting the data table of tainan, it can be seen that the counties data is found in the TOWNID column, so I will be filtering them from there.\n\nstudy_area <- taiwan %>%\n              filter(TOWNID == \"D01\" | TOWNID == \"D02\" | TOWNID == \"D04\" |\n                       TOWNID == \"D06\" | TOWNID == \"D07\" | TOWNID == \"D08\" |\n                       TOWNID == \"D32\" | TOWNID == \"D39\")\n\n\nplot(study_area[\"TOWNID\"])\n\nNow, we can check for any missing geospatial data from the desired study areas.\n\nany(is.na(study_area$geometry))\n\nWith these checks done, we can save the data in the rds format to allow for faster data retrieval.\n\nwrite_rds(study_area, \"data/rds/tainan_study.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#aspatial-data",
    "href": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#aspatial-data",
    "title": "Take-home Exercise 2",
    "section": "Aspatial data",
    "text": "Aspatial data\nFor aspatial data, I will be using read_csv() to import the csv file Dengue_Daily, which contains the aspatial data of reported dengue cases in Taiwan.\n\ndisease_data <- read_csv(\"data/aspatial/Dengue_Daily.csv\")\n\nAgain, the data is not limited to only the city of Tainan. Before cleaning, I inspected the data using the tab on the side (the data is large and difficult to examine using methods such as head()) and found that almost all of the data and all of the attribute columns are written in Simplified Chinese.\nRight now, I want to narrow the data to only reported cases within the city of Tainan. To do that, I will be using filter() to retrieve all data that are labelled as 台南市 in the attribute column 居住縣市\n\ntainan_cases <- disease_data %>%\n              filter(居住縣市 == \"台南市\")\n\nWith this narrower list, we can look at the attribute columns. For this exercise, the most important ones are the following:\n\n\n\nAttribute\nTranslation\n\n\n\n\n發病日\nOnset date\n\n\n最小統計區中心點X\nX-coordinate\n\n\n最小統計區中心點Y\nY-coordinate\n\n\n\n\nEpidemiology week - research\n\nThe exercise calls for the use of epidemiology weeks, but what are they?\nAccording to the Central Massachusetts Mosquito Control Project (CMMCP), epidemiological weeks are “a standardized method of counting weeks to allow for the comparison of data year after year”. By default, Sunday is marked as the beginning of an epidemiology week and Saturday is the end of an epidemiology week.\nBut why use epidemiological weeks? CMMCP explains that for certain types of data, such as mosquito surveillance, “daily increments are too frequent and too varied to be able to be managed and analyzed, or there are many factors that make it impossible to compare daily results”, and frequent data interpretation is required. Hence, an epidemiological week creates a form of intermediary period of time that allows for data analysis to occur."
  },
  {
    "objectID": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#data-cleaning-continued",
    "href": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#data-cleaning-continued",
    "title": "Take-home Exercise 2",
    "section": "Data cleaning continued",
    "text": "Data cleaning continued\nThe epiweek() functions from the lubridate package is able to return the epidemiological week from a dataset. Instead of creating a new dataset, we can instead create a new attribute column titled “EpiWk”.\n\ntainan_cases$EpiWk <- epiweek(tainan_cases$發病日)\n\nWith that accomplished, we can filter for the time frame given in the exercise, which is epidemiological weeks 31-50 of the year 2023.\n\nfinal_cases <- subset(tainan_cases, substr(發病日, 0,4) == \"2023\")\nfinal_cases <- final_cases %>% filter(EpiWk >= 31 & EpiWk <= 50)\n\nI will also check for any missing geographical data.\n\nany(is.na(final_cases$最小統計區中心點X))\nany(is.na(final_cases$最小統計區中心點Y))\n\nDespite the code showing us that there are no missing values, checking the data table shows that there are 10 items without x or y coordinates. These values need to be removed to prevent issues in the future.\n\nclean_data <- final_cases %>%\n  filter(!(最小統計區中心點X=='None' | 最小統計區中心點Y > 'None'))\n\n\nany(is.na(clean_data$最小統計區中心點X))\nany(is.na(clean_data$最小統計區中心點Y))\n\nWe can now write this cleaned data out as an rds.\n\nwrite_rds(clean_data, \"data/rds/cases_cleaned.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#brief-exploratory-data-analysis-cases-over-time",
    "href": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#brief-exploratory-data-analysis-cases-over-time",
    "title": "Take-home Exercise 2",
    "section": "Brief exploratory data analysis: cases over time",
    "text": "Brief exploratory data analysis: cases over time\nTo visualise the spread of cases over epidemiological weeks 31-50, we can use the hist() function.\n\ncases_num <- as.numeric(cases$EpiWk)\n\nhist(cases_num, xlim = range(30, 50), xlab = \"Epidemiological Week\", ylab = \"Frequency\", main = \"Distribution of Cases over Time (2023)\", col = \"blue\")\n\n\n\n\nThis histogram tells us that the number of dengue cases is normally distributed over epidemiological weeks 31-50."
  },
  {
    "objectID": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#checking-for-null-values",
    "href": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#checking-for-null-values",
    "title": "Take-home Exercise 2",
    "section": "Checking for null values",
    "text": "Checking for null values\nBefore doing anything further, I am going to check for null values in the geometry column to ensure that everything lies within the map and that nothing was lost during the join.\n\nany(is.na(tainan_consol$geometry))\n\n[1] FALSE"
  },
  {
    "objectID": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#computing-sum-of-all-cases",
    "href": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#computing-sum-of-all-cases",
    "title": "Take-home Exercise 2: Technical Guide",
    "section": "Computing sum of all cases",
    "text": "Computing sum of all cases\nI will also need to compute the sum of all cases in that region for future chloropleth mapping including transforming it into an sf object.\n\ntainan_grouped <- tainan_consol %>%\n                  group_by(居住鄉鎮, 居住村里) %>%\n                  summarise(count=n())\n\ntainan_grouped <- left_join(tainan_grouped, tainan_consol)\ntainan_grouped <- st_as_sf(tainan_grouped, coords = c(\"最小統計區中心點X\", \"最小統計區中心點Y\"), crs = st_crs(tainan))"
  },
  {
    "objectID": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#computing-sum-of-cases-by-village",
    "href": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#computing-sum-of-cases-by-village",
    "title": "Take-home Exercise 2",
    "section": "Computing sum of cases by village",
    "text": "Computing sum of cases by village\nNext, I will also need to compute the total sum of cases by village and week.\n\nsummed_cases <- tainan_consol %>%\n  group_by(TOWNVILL, EpiWk) %>%\n  summarise(weekly_cases=n()) %>%\n  st_drop_geometry()\n\nHowever, doing this will result in weeks with 0 cases being dropped. To fix this, we will have to rejoin the missing village data.\n\n#Creating TOWNVILL column to allow for extraction\ntainan$TOWNVILL = paste(tainan$居住鄉鎮, tainan$居住村里)\n  \n#Extracting all unique TOWNVILL with EpiWk\nunique_identity <- expand_grid(TOWNVILL = unique(tainan$TOWNVILL),\n                               EpiWk = 31:50)\n\n#Join to summed_cases\nsummed_cases <- left_join(unique_identity, summed_cases, by = c(\"TOWNVILL\", \"EpiWk\"))\n\n#Replace missing \"NA\" values with 0\nsummed_cases_final <- mutate(summed_cases, \n                             weekly_cases = if_else(is.na(weekly_cases), 0, weekly_cases))"
  },
  {
    "objectID": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#visualisation-using-chloropleth-map",
    "href": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#visualisation-using-chloropleth-map",
    "title": "Take-home Exercise 2",
    "section": "Visualisation using chloropleth map",
    "text": "Visualisation using chloropleth map\nFirst, I want to see what the map may look like using a quantile distribution. To do that, I will need to join the attributes in summed_cases_final and tainan together.\n\nsummed_sf <- left_join(tainan, summed_cases_final)\n\n\ntmap_mode('plot')\n\ntm_shape(summed_sf) +\n  tm_fill(\"weekly_cases\",\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_layout(main.title = \"Distribution of cases\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\ntmap_mode('view')\n\nAs we can see, the quantile distribution is positively skewed, with multiple bins starting at 0. This suggests that there may be clustering for dengue cases."
  },
  {
    "objectID": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#global-spatial-autocorrelation",
    "href": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#global-spatial-autocorrelation",
    "title": "Take-home Exercise 2",
    "section": "Global Spatial Autocorrelation",
    "text": "Global Spatial Autocorrelation\n\nCreating contiguity weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units in the study area.\n\nwm_q <- summed_sf %>%\n  mutate( nb = st_contiguity(geometry),\n          wt = st_weights(nb, style= 'W'),\n          .before = 1)\n\n\n\nComputing Global Moran’s I permutation test\nInstead of using the standard Moran’s I test, which can only give us the p value and on its own is not important without confidence, I have chosen to use Moran’s I permutation test. This test involves simulation, where more iterations to backup the analysis and confirm the representativeness of the dataset.\nBefore performing the simulation, I am going to set the seed using set_seed() to ensure that the results are reproducible. I will be running nsim = 99 simulations.\n\nset.seed(1234)\n\n\nglobal_moran_perm(wm_q$weekly_cases,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.15977, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\n\nFrom the p-value returned, I can reject the null hypothesis that the distribution of dengue cases is even. It can be inferred that there is clustering (the statistic is more than 0). This method is most convincing overall."
  },
  {
    "objectID": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#local-spatial-autocorrelation",
    "href": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#local-spatial-autocorrelation",
    "title": "Take-home Exercise 2",
    "section": "Local Spatial Autocorrelation",
    "text": "Local Spatial Autocorrelation\n\nComputing local Moran’s I\nThe decomposition of global Moran’s I can be used to help detect clusters and outliers.\n\nlisa <- wm_q %>% \n  mutate(local_moran = local_moran(\n    weekly_cases, nb, wt, nsim = 99),\n         .before = 1) %>%\n  unnest(local_moran)\n\n\n\nMapping local Moran’s I\nLocal Moran’s I results can be plotted on a chloropleth map. Using chloropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the following code.\n\ntmap_mode('plot')\ntm_shape(lisa) +\n  tm_fill(col = \"ii\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran's I for dengue cases in Tainan\",\n            main.title.size = 0.8)\n\n\n\ntmap_mode('view')\n\n\n\nMapping both local Moran’s I values and p-values\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\n\nlocalMI.map <- tm_shape(lisa) +\n  tm_fill(col = \"ii\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran's I\",\n            main.title.size = 0.8)\n\npvalue.map <- tm_shape(lisa) +\n  tm_fill(col = \"p_ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf)) +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran's I p-values\",\n            main.title.size = 0.8) +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMapping LISA map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. To prepare a LISA map, I use the following code:\n\n#filter for statistically significant clusters\nlisa_sig <- lisa %>% filter(p_ii < 0.5)\n\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.4) +\ntm_layout(main.title = \"LISA map\",\n            main.title.size = 0.8)\n\n\n\ntmap_mode('view')"
  },
  {
    "objectID": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#building-timespace-cube",
    "href": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#building-timespace-cube",
    "title": "Take-home Exercise 2",
    "section": "Building timespace cube",
    "text": "Building timespace cube\n\ndengue_cube <- spacetime(summed_cases_final, tainan,\n                      .loc_col = \"TOWNVILL\",\n                      .time_col = \"EpiWk\")\n\nis_spacetime_cube(dengue_cube)\n\n[1] TRUE"
  },
  {
    "objectID": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#emerging-hot-spot-analysis-ehsa",
    "href": "Take-home_Ex/Take-Home_Ex02/Take-home_Ex02a.html#emerging-hot-spot-analysis-ehsa",
    "title": "Take-home Exercise 2",
    "section": "Emerging Hot Spot Analysis (EHSA)",
    "text": "Emerging Hot Spot Analysis (EHSA)\n\nComputing Local Gi* statistics\nSpecially calibrated to detect hot and cold spots. By providing the desired weights and using inverse distance there is more weight given to neighbours that are closer based on distance.\n\nwm_idw <- summed_sf %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb, geometry,\n                                   scale = 1,\n                                   alpha = 1),\n         .before = 1)\n\n\n\nBuilding timespace cube\nI will next build a timespace cube using summed_cases_final and tainan data sets.\n\ndengue_cube <- spacetime(summed_cases_final, tainan,\n                      .loc_col = \"TOWNVILL\",\n                      .time_col = \"EpiWk\")\n\nis_spacetime_cube(dengue_cube)\n\n[1] TRUE\n\n\n\n\nDeriving spatial weights\nbreak\n\ndengue_nb <- dengue_cube %>%\n  activate(\"geometry\")  %>% #only pulls out the geometry layer\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %>%\n  set_nbs(\"nb\") %>%\n  set_wts(\"wt\")\n\nNote that this dataset now has neighbors and weights for each time-slice.\n\nhead(dengue_nb)\n\n# A tibble: 6 × 5\n  TOWNVILL      EpiWk weekly_cases nb        wt       \n  <chr>         <dbl>        <dbl> <list>    <list>   \n1 安南區 青草里    31            0 <int [4]> <dbl [4]>\n2 仁德區 保安里    31            1 <int [6]> <dbl [6]>\n3 中西區 赤嵌里    31            0 <int [9]> <dbl [9]>\n4 南區 大成里      31            0 <int [7]> <dbl [7]>\n5 安南區 城北里    31            0 <int [5]> <dbl [5]>\n6 安南區 城南里    31            0 <int [8]> <dbl [8]>\n\n\nWe can use these new columns to manually calculate the local Gi* for each location.\n\ngi_stars <- dengue_nb %>% \n  group_by(EpiWk) %>% \n  mutate(gi_star = local_gstar_perm(\n    weekly_cases, nb, wt)) %>% \n  tidyr::unnest(gi_star)\n\n\n\nMann-Kendall Method\nThis helps us to detect the statistical significance of each location. It is a non-parametric method.\nWith these Gi* measures we can then evaluate each location for a trend using the Mann-Kendall test and test its significance.\n\nehsa <- gi_stars %>%\n  group_by(EpiWk) %>%\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %>%\n  tidyr::unnest_wider(mk)\n\nNext, it can be arranged to show signficant hot and cold spots.\n\nemerging <- ehsa %>% \n  arrange(sl, abs(tau)) %>% \n  slice(1:5)\n\nhead(emerging)\n\n# A tibble: 5 × 6\n  EpiWk    tau         sl     S      D     varS\n  <dbl>  <dbl>      <dbl> <dbl>  <dbl>    <dbl>\n1    36 -0.185 0.00000942 -6138 33128. 1919134.\n2    35 -0.183 0.0000127  -6049 33107. 1919078.\n3    32 -0.182 0.0000143  -6012 33002. 1918589.\n4    42 -0.165 0.0000786  -5471 33117. 1919110.\n5    39 -0.155 0.000203   -5147 33128. 1919137.\n\n\n\n\nMapping EHSA\nFinally, we can conduct EHSA analysis. There is a function called emerging_hotspot_analysis() from the sfdep package. It combines the Getis-Ord Gi* statistic with the Mann-Kendall trend test to determine if there is a temporal trend associated with local clustering of hot and cold spots.\nThe arguments are as follows:\n\nx: a spacetime object, must be a spacetime cube\n.var: a numeric vector in the spacetime cube with no missing values\nk: defaults to 1. The number of time lags to include in the neighborhood for calculating the local Gi*\nnsim: default 199. The number of simulations to run in calculating the simulated p-value for the local Gi*\n\n\nehsa <- emerging_hotspot_analysis(\n  x = dengue_cube, \n  .var = \"weekly_cases\", \n  k = 1, \n  nsim = 99\n)\n\nBefore EHSA can be mapped, we need to join them with summed_sf (for the geometry)\n\ntainan_ehsa <- summed_sf %>%\n  left_join(ehsa,\n            by = join_by(TOWNVILL == location))\n\nNow, I can plot the EHSA of dengue cases in Tainan.\n\nehsa_sig <- tainan_ehsa  %>%\n  filter(p_value < 0.05)\n\n\ntmap_mode(\"plot\")\ntm_shape(tainan_ehsa) +\n  tm_polygons() +\n  tm_borders(0.8) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") +\n  tm_borders(0.3) +\ntm_layout(main.title = \"Emerging Hotspot and Coldspots\",\n            main.title.size = 0.8)\n\n\n\ntmap_mode('view')\n\nWe can also visualise the distribution of hot and cold spots via ggplot.\n\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))"
  }
]