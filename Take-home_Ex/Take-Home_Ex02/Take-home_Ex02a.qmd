---
title: "Take-home Exercise 2: Technical Guide"

execute:
  warning: false
  freeze: false
date: "`r Sys.Date()`"
---

# Project Objectives

For this geospatial task, we have been tasked to discover:

-   if the distribution of dengue fever outbreak at Tainan City, Taiwan are independent from space and space and time.

-   If the outbreak is indeed spatial and spatio-temporal dependent, then, you would like to detect where are the clusters and outliers, and the emerging hot spot/cold spot areas.

# Installing R packages

For the technical portion of the take home exercise, I will be using the following packages:

-   tidyverse

-   sf

-   sfdep

-   tmap

-   readr

```{r}
pacman::p_load(tidyverse, sf, sfdep, tmap, readr)
```

# Data import and cleaning

## Geospatial data

For geospatial data, using *st_read()*, I will import the geospatial data file `TAINAN_VILLAGE`, which contains the geospatial data of village boundary of Taiwan.

```{r}
#| eval: false

taiwan <- st_read(dsn = "data/geospatial", 
                layer = "TAINAN_VILLAGE")
```

::: callout-note
The data is in Taiwan Geographic Coordinate System, TWD97. This should be kept unchanged to ensure that the data is as accurate as possible to the collected data.
:::

Now the map can be plotted so that we can see what we are working with, grouped by `TOWNID` column.

```{r}
#| eval: false

plot(taiwan["TOWNID"])
```

The geospatial data here is not limited to only the city of Tainan. To filter out Tainan City for the use of this exercise, I will filter for the following counties:

-   D01

-   D02

-   D04

-   D06

-   D07

-   D08

-   D32

-   D39

This will make the later parts of data cleaning faster due to having less to process.

From inspecting the data table of `tainan`, it can be seen that the counties data is found in the **TOWNID** column, so I will be filtering them from there.

```{r}
#| eval: false

study_area <- taiwan %>%
              filter(TOWNID == "D01" | TOWNID == "D02" | TOWNID == "D04" |
                       TOWNID == "D06" | TOWNID == "D07" | TOWNID == "D08" |
                       TOWNID == "D32" | TOWNID == "D39")


plot(study_area["TOWNID"])
```

Now, we can check for any missing geospatial data from the desired study areas.

```{r}
#| eval: false

any(is.na(study_area$geometry))
```

With these checks done, we can save the data in the rds format to allow for faster data retrieval.

```{r}
#| eval: false

write_rds(study_area, "data/rds/tainan_study.rds")
```

## Aspatial data

For aspatial data, I will be using *read_csv()* to import the csv file `Dengue_Daily`, which contains the aspatial data of reported dengue cases in Taiwan.

```{r}
#| eval: false

disease_data <- read_csv("data/aspatial/Dengue_Daily.csv")
```

Again, the data is not limited to only the city of Tainan. Before cleaning, I inspected the data using the tab on the side (the data is large and difficult to examine using methods such as *head()*) and found that almost all of the data and all of the attribute columns are written in Simplified Chinese.

Right now, I want to narrow the data to only reported cases within the city of Tainan. To do that, I will be using *filter()* to retrieve all data that are labelled as 台南市 in the attribute column `居住縣市`

```{r}
#| eval: false

tainan_cases <- disease_data %>%
              filter(居住縣市 == "台南市")
```

With this narrower list, we can look at the attribute columns. For this exercise, the most important ones are the following:

| Attribute         | Translation  |
|-------------------|--------------|
| 發病日            | Onset date   |
| 最小統計區中心點X | X-coordinate |
| 最小統計區中心點Y | Y-coordinate |

### Epidemiology week - research

> The exercise calls for the use of epidemiology weeks, but what are they?
>
> According to the Central Massachusetts Mosquito Control Project (CMMCP), epidemiological weeks are ["a standardized method of counting weeks to allow for the comparison of data year after year"](https://www.cmmcp.org/mosquito-surveillance-data/pages/epi-week-calendars-2008-2024). By default, **Sunday** is marked as the beginning of an epidemiology week and Saturday is the end of an epidemiology week.
>
> But why use epidemiological weeks? CMMCP explains that for certain types of data, such as mosquito surveillance, "[daily increments are too frequent and too varied to be able to be managed and analyzed, or there are many factors that make it impossible to compare daily results](https://www.cmmcp.org/mosquito-surveillance-data/pages/epi-week-calendars-2008-2024)", and frequent data interpretation is required. Hence, an epidemiological week creates a form of intermediary period of time that allows for data analysis to occur.

## Data cleaning continued

The [*epiweek()*](https://lubridate.tidyverse.org/reference/week.html#ref-examples) functions from the lubridate package is able to return the epidemiological week from a dataset. Instead of creating a new dataset, we can instead create a new attribute column titled "EpiWk".

```{r}
#| eval: false

tainan_cases$EpiWk <- epiweek(tainan_cases$發病日)
```

With that accomplished, we can filter for the time frame given in the exercise, which is epidemiological weeks 31-50 of the year 2023.

```{r}
#| eval: false

final_cases <- subset(tainan_cases, substr(發病日, 0,4) == "2023")
final_cases <- final_cases %>% filter(EpiWk >= 31 & EpiWk <= 50)
```

We can now write this cleaned data out as an rds.

```{r}
#| eval: false

write_rds(final_cases, "data/rds/cases_cleaned.rds")
```

# Importing required data

```{r}
tainan <- read_rds("data/rds/tainan_study.rds")
cases <- read_rds("data/rds/cases_cleaned.rds")
```
